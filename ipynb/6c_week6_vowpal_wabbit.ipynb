{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>\n",
    "## Специализация \"Машинное обучение и анализ данных\"\n",
    "<center>Автор материала: программист-исследователь Mail.Ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ [Юрий Кашницкий](https://yorko.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone проект №1 <br> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 6.  Vowpal Wabbit\n",
    "\n",
    "На этой неделе мы познакомимся с популярной библиотекой Vowpal Wabbit и попробуем ее на данных по посещению сайтов.\n",
    "\n",
    "**План 6 недели:**\n",
    "- Часть 1. Статья по Vowpal Wabbit\n",
    "- Часть 2. Применение Vowpal Wabbit к данным по посещению сайтов\n",
    " - 2.1. Подготовка данных\n",
    " - 2.2. Валидация по отложенной выборке\n",
    " - 2.3. Валидация по тестовой выборке (Public Leaderboard)\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. `sklearn.linear_model`. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "Также будет полезна [презентация](https://github.com/esokolov/ml-course-msu/blob/master/ML15/lecture-notes/Sem08_vw.pdf) лектора специализации Евгения Соколова. И, конечно же, [документация](https://github.com/JohnLangford/vowpal_wabbit/wiki) Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "1. Заполните код в этой тетрадке \n",
    "2. Если вы проходите специализацию Яндеса и МФТИ, пошлите файл с ответами в соответствующем Programming Assignment. <br> Если вы проходите курс ODS, выберите ответы в [веб-форме](https://docs.google.com/forms/d/1wteunpEhAt_9s-WBwxYphB6XpniXsAZiFSNuFNmvOdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1. Статья про Vowpal Wabbit\n",
    "Прочитайте [статью](https://habrahabr.ru/company/ods/blog/326418/) про Vowpal Wabbit на Хабре из серии открытого курса OpenDataScience по машинному обучению. Материал для этой статьи зародился из нашей специализации. Скачайте [тетрадку](https://github.com/Yorko/mlcourse_open/blob/master/jupyter_russian/topic08_sgd_hashing_vowpal_wabbit/topic8_sgd_hashing_vowpal_wabbit.ipynb), прилагаемую к статье, посмотрите код, изучите его, поменяйте, только так можно разобраться с Vowpal Wabbit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Применение Vowpal Wabbit к данным по посещению сайтов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Далее посмотрим на Vowpal Wabbit в деле. Правда, в задаче нашего соревнования при бинарной классификации веб-сессий мы разницы не заметим – как по качеству, так и по скорости работы (хотя можете проверить), продемонстрируем всю резвость VW в задаче классификации на 400 классов. Исходные данные все те же самые, но выделено 400 пользователей, и решается задача их идентификации. Скачайте данные [отсюда](https://inclass.kaggle.com/c/identify-me-if-you-can4/data) – файлы `train_sessions_400users.csv` и `test_sessions_400users.csv`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = '/home/truename/Documents/capstone_user_identification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузим обучающую и тестовую выборки. Можете заметить, что тестовые сессии здесь по времени четко отделены от сессий в обучающей выборке. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'train_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df_400 = pd.read_csv(os.path.join(PATH_TO_DATA,'test_sessions_400users.csv'), \n",
    "                           index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>2014-03-24 15:22:40</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-24 15:22:55</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:01</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:03</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:04</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:05</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>2014-04-17 14:25:58</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>665.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>8727.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:18</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:47</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:48</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>2014-03-21 10:12:24</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-03-21 10:12:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:12:54</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:24</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-21 10:13:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:54</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2014-03-21 10:14:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:06</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>2013-12-13 09:52:28</td>\n",
       "      <td>925.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-11-26 12:35:29</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:04</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1    site2                time2    site3  \\\n",
       "session_id                                                                      \n",
       "1           23713  2014-03-24 15:22:40  23720.0  2014-03-24 15:22:48  23713.0   \n",
       "2            8726  2014-04-17 14:25:58   8725.0  2014-04-17 14:25:59    665.0   \n",
       "3             303  2014-03-21 10:12:24     19.0  2014-03-21 10:12:36    303.0   \n",
       "4            1359  2013-12-13 09:52:28    925.0  2013-12-13 09:54:34   1240.0   \n",
       "5              11  2013-11-26 12:35:29     85.0  2013-11-26 12:35:31     52.0   \n",
       "\n",
       "                          time3    site4                time4    site5  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:22:48  23713.0  2014-03-24 15:22:54  23720.0   \n",
       "2           2014-04-17 14:25:59   8727.0  2014-04-17 14:25:59     45.0   \n",
       "3           2014-03-21 10:12:54    303.0  2014-03-21 10:13:01    303.0   \n",
       "4           2013-12-13 09:54:34   1360.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:35:31     85.0  2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time5   ...                  time6    site7  \\\n",
       "session_id                        ...                                   \n",
       "1           2014-03-24 15:22:54   ...    2014-03-24 15:22:55  23713.0   \n",
       "2           2014-04-17 14:25:59   ...    2014-04-17 14:26:01     45.0   \n",
       "3           2014-03-21 10:13:24   ...    2014-03-21 10:13:36    303.0   \n",
       "4           2013-12-13 09:54:34   ...    2013-12-13 09:54:34   1346.0   \n",
       "5           2013-11-26 12:35:32   ...    2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:23:01  23713.0  2014-03-24 15:23:03  23713.0   \n",
       "2           2014-04-17 14:26:01   5320.0  2014-04-17 14:26:18   5320.0   \n",
       "3           2014-03-21 10:13:54    309.0  2014-03-21 10:14:01    303.0   \n",
       "4           2013-12-13 09:54:34   1345.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:03     10.0   \n",
       "\n",
       "                          time9   site10               time10 user_id  \n",
       "session_id                                                             \n",
       "1           2014-03-24 15:23:04  23713.0  2014-03-24 15:23:05     653  \n",
       "2           2014-04-17 14:26:47   5320.0  2014-04-17 14:26:48     198  \n",
       "3           2014-03-21 10:14:06    303.0  2014-03-21 10:14:24      34  \n",
       "4           2013-12-13 09:58:19   1345.0  2013-12-13 09:58:19     601  \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:04     273  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Видим, что в обучающей выборке 182793 сессий, в тестовой – 46473, и сессии действительно принадлежат 400 различным пользователям.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((182793, 21), (46473, 20), 400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_400.shape, test_df_400.shape, train_df_400['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vowpal Wabbit любит, чтоб метки классов были распределены от 1 до K, где K – число классов в задаче классификации (в нашем случае – 400). Поэтому придется применить `LabelEncoder`, да еще и +1 потом добавить (`LabelEncoder` переводит метки в диапозон от 0 до K-1). Потом надо будет применить обратное преобразование.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([262,  82,  16, ..., 254,  91, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train_df_400['user_id']\n",
    "class_encoder = LabelEncoder()\n",
    "y_for_vw = class_encoder.fit_transform(y)+1\n",
    "y_for_vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Далее будем сравнивать VW с SGDClassifier и с логистической регрессией. Всем моделям этим нужна предобработка входных данных. Подготовьте для sklearn-моделей разреженные матрицы, как мы это делали в 5 части:**\n",
    "- объедините обучающиую и тестовую выборки\n",
    "- выберите только сайты (признаки от 'site1' до 'site10')\n",
    "- замените пропуски на нули (сайты у нас нумеровались с 0)\n",
    "- переведите в разреженный формат `csr_matrix`\n",
    "- разбейте обратно на обучающую и тестовую части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sites = ['site' + str(i) for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df_400, test_df_400])\n",
    "def sparse_df(df):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    X = df.values\n",
    "    for d in X:\n",
    "        for term in d:\n",
    "            index = vocabulary.setdefault(term, len(vocabulary))\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((data, indices, indptr), dtype=int)[:,1:]\n",
    "\n",
    "train_test_df_sites = train_test_df[sites].fillna(0).astype('int')\n",
    "\n",
    "train_test_sparse = sparse_df(train_test_df_sites)\n",
    "train_shares = train_df_400.shape[0]\n",
    "X_train_sparse = train_test_sparse[:train_shares, :]\n",
    "X_test_sparse = train_test_sparse[train_shares:, :]\n",
    "y = train_df_400['user_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Валидация по отложенной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выделим обучающую (70%) и отложенную (30%) части исходной обучающей выборки. Данные не перемешиваем, учитываем, что сессии отсортированы по времени.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_share = int(.7 * train_df_400.shape[0])\n",
    "train_df_part = train_df_400[sites].iloc[:train_share, :]\n",
    "valid_df = train_df_400[sites].iloc[train_share:, :]\n",
    "X_train_part_sparse = X_train_sparse[:train_share, :]\n",
    "X_valid_sparse = X_train_sparse[train_share:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_part = y[:train_share]\n",
    "y_valid = y[train_share:]\n",
    "y_train_part_for_vw = y_for_vw[:train_share]\n",
    "y_valid_for_vw = y_for_vw[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализуйте функцию, `arrays_to_vw`, переводящую обучающую выборку в формат Vowpal Wabbit.**\n",
    "\n",
    "Вход:\n",
    " - X – матрица `NumPy` (обучающая выборка)\n",
    " - y (необяз.) - вектор ответов (`NumPy`). Необязателен, поскольку тестовую матрицу будем обрабатывать этой же функцией\n",
    " - train – флаг, True в случае обучающей выборки, False – в случае тестовой выборки\n",
    " - out_file – путь к файлу .vw, в который будет произведена запись\n",
    " \n",
    "Детали:\n",
    "- надо пройтись по всем строкам матрицы `X` и записать через пробел все значения, предварительно добавив вперед нужную метку класса из вектора `y` и знак-разделитель `|`\n",
    "- в тестовой выборке на месте меток целевого класса можно писать произвольные, допустим, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arrays_to_vw(X, y=None, train=True, out_file='tmp.vw'):\n",
    "    def to_vw_format(Xrow, yrow=None):\n",
    "        return str(yrow or '1') + ' | ' + ' '.join(Xrow) + '\\n'\n",
    "    with open(out_file, 'w') as vw_train_data:\n",
    "        for row in range(X.shape[0]):\n",
    "            if train:\n",
    "                vw_train_data.write(to_vw_format([str(i) for i in X.iloc[row,:].fillna(0).astype('int')], y[row]))\n",
    "            else:\n",
    "                vw_train_data.write(to_vw_format([str(i) for i in X.iloc[row,:].fillna(0).astype('int')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примените написанную функцию к части обучащей выборки `(train_df_part, y_train_part_for_vw)`, к отложенной выборке `(valid_df, y_valid_for_vw)`, ко всей обучающей выборке и ко всей тестовой выборке. Обратите внимание, что на вход наш метод принимает именно матрицы и вектора `NumPy`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 160 ms, total: 2min 52s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# будет 4 вызова\n",
    "arrays_to_vw(train_df_part, y_train_part_for_vw, True, PATH_TO_DATA+'/train_part.vw')\n",
    "arrays_to_vw(valid_df, y_valid_for_vw, True, PATH_TO_DATA+'/valid.vw')\n",
    "arrays_to_vw(train_df_400[sites], y_for_vw, True, PATH_TO_DATA+'/train.vw')\n",
    "arrays_to_vw(test_df_400[sites], None, False, PATH_TO_DATA+'/test.vw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 9 304 308 307 91 308 312 300 305 309\n",
      "1 | 838 504 68 11 838 11 838 886 27 305\n",
      "1 | 190 192 8 189 191 189 190 2375 192 8\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/test.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результат должен получиться таким.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 | 23713 23720 23713 23713 23720 23713 23713 23713 23713 23713\n",
      "82 | 8726 8725 665 8727 45 8725 45 5320 5320 5320\n",
      "16 | 303 19 303 303 303 303 303 309 303 303\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/train_part.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 | 7 923 923 923 11 924 7 924 838 7\n",
      "160 | 91 198 11 11 302 91 668 311 310 91\n",
      "312 | 27085 848 118 118 118 118 11 118 118 118\n"
     ]
    }
   ],
   "source": [
    "!head -3  $PATH_TO_DATA/valid.vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | 9 304 308 307 91 308 312 300 305 309\n",
      "1 | 838 504 68 11 838 11 838 886 27 305\n",
      "1 | 190 192 8 189 191 189 190 2375 192 8\n"
     ]
    }
   ],
   "source": [
    "!head -3 $PATH_TO_DATA/test.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучите модель Vowpal Wabbitна выборке `train_part.vw`. Укажите, что решается задача классификации с 400 классами (`--oaa`), сделайте 3 прохода по выборке (`--passes`). Задайте некоторый кэш-файл (`--cache_file`, можно просто указать флаг `-c`), так VW будет быстрее делать все следующие после первого проходы по выборке (прошлый кэш-файл удаляется с помощью аргумента `-k`). Также укажите значение параметра `b`=26. Это число бит, используемых для хэширования, в данном случае нужно больше, чем 18 по умолчанию. Наконец, укажите `random_seed`=17. Остальные параметры пока не меняйте, далее уже в свободном режиме соревнования можете попробовать другие функции потерь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_part_vw = os.path.join(PATH_TO_DATA, 'train_part.vw')\n",
    "valid_vw = os.path.join(PATH_TO_DATA, 'valid.vw')\n",
    "train_vw = os.path.join(PATH_TO_DATA, 'train.vw')\n",
    "test_vw = os.path.join(PATH_TO_DATA, 'test.vw')\n",
    "model = os.path.join(PATH_TO_DATA, 'vw_model.vw')\n",
    "pred = os.path.join(PATH_TO_DATA, 'vw_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = \n",
      "num sources = 1\n",
      "\n",
      "\n",
      "VW options:\n",
      "  --random_seed arg                     seed random number generator\n",
      "  --ring_size arg                       size of example ring\n",
      "\n",
      "Update options:\n",
      "  -l [ --learning_rate ] arg            Set learning rate\n",
      "  --power_t arg                         t power value\n",
      "  --decay_learning_rate arg             Set Decay factor for learning_rate \n",
      "                                        between passes\n",
      "  --initial_t arg                       initial t value\n",
      "  --feature_mask arg                    Use existing regressor to determine \n",
      "                                        which parameters may be updated.  If no\n",
      "                                        initial_regressor given, also used for \n",
      "                                        initial weights.\n",
      "\n",
      "Weight options:\n",
      "  -i [ --initial_regressor ] arg        Initial regressor(s)\n",
      "  --initial_weight arg                  Set all weights to an initial value of \n",
      "                                        arg.\n",
      "  --random_weights arg                  make initial weights random\n",
      "  --input_feature_regularizer arg       Per feature regularization input file\n",
      "\n",
      "Parallelization options:\n",
      "  --span_server arg                     Location of server for setting up \n",
      "                                        spanning tree\n",
      "  --threads                             Enable multi-threading\n",
      "  --unique_id arg (=0)                  unique id used for cluster parallel \n",
      "                                        jobs\n",
      "  --total arg (=1)                      total number of nodes used in cluster \n",
      "                                        parallel job\n",
      "  --node arg (=0)                       node number in cluster parallel job\n",
      "\n",
      "Diagnostic options:\n",
      "  --version                             Version information\n",
      "  -a [ --audit ]                        print weights of features\n",
      "  -P [ --progress ] arg                 Progress update frequency. int: \n",
      "                                        additive, float: multiplicative\n",
      "  --quiet                               Don't output disgnostics and progress \n",
      "                                        updates\n",
      "  -h [ --help ]                         Look here: http://hunch.net/~vw/ and \n",
      "                                        click on Tutorial.\n",
      "\n",
      "Feature options:\n",
      "  --hash arg                            how to hash the features. Available \n",
      "                                        options: strings, all\n",
      "  --ignore arg                          ignore namespaces beginning with \n",
      "                                        character <arg>\n",
      "  --keep arg                            keep namespaces beginning with \n",
      "                                        character <arg>\n",
      "  --redefine arg                        redefine namespaces beginning with \n",
      "                                        characters of string S as namespace N. \n",
      "                                        <arg> shall be in form 'N:=S' where := \n",
      "                                        is operator. Empty N or S are treated \n",
      "                                        as default namespace. Use ':' as a \n",
      "                                        wildcard in S.\n",
      "  -b [ --bit_precision ] arg            number of bits in the feature table\n",
      "  --noconstant                          Don't add a constant feature\n",
      "  -C [ --constant ] arg                 Set initial value of constant\n",
      "  --ngram arg                           Generate N grams. To generate N grams \n",
      "                                        for a single namespace 'foo', arg \n",
      "                                        should be fN.\n",
      "  --skips arg                           Generate skips in N grams. This in \n",
      "                                        conjunction with the ngram tag can be \n",
      "                                        used to generate generalized \n",
      "                                        n-skip-k-gram. To generate n-skips for \n",
      "                                        a single namespace 'foo', arg should be\n",
      "                                        fN.\n",
      "  --feature_limit arg                   limit to N features. To apply to a \n",
      "                                        single namespace 'foo', arg should be \n",
      "                                        fN\n",
      "  --affix arg                           generate prefixes/suffixes of features;\n",
      "                                        argument '+2a,-3b,+1' means generate \n",
      "                                        2-char prefixes for namespace a, 3-char\n",
      "                                        suffixes for b and 1 char prefixes for \n",
      "                                        default namespace\n",
      "  --spelling arg                        compute spelling features for a give \n",
      "                                        namespace (use '_' for default \n",
      "                                        namespace)\n",
      "  --dictionary arg                      read a dictionary for additional \n",
      "                                        features (arg either 'x:file' or just \n",
      "                                        'file')\n",
      "  --dictionary_path arg                 look in this directory for \n",
      "                                        dictionaries; defaults to current \n",
      "                                        directory or env{PATH}\n",
      "  --interactions arg                    Create feature interactions of any \n",
      "                                        level between namespaces.\n",
      "  --permutations                        Use permutations instead of \n",
      "                                        combinations for feature interactions \n",
      "                                        of same namespace.\n",
      "  --leave_duplicate_interactions        Don't remove interactions with \n",
      "                                        duplicate combinations of namespaces. \n",
      "                                        For ex. this is a duplicate: '-q ab -q \n",
      "                                        ba' and a lot more in '-q ::'.\n",
      "  -q [ --quadratic ] arg                Create and use quadratic features\n",
      "  --q: arg                              : corresponds to a wildcard for all \n",
      "                                        printable characters\n",
      "  --cubic arg                           Create and use cubic features\n",
      "\n",
      "Example options:\n",
      "  -t [ --testonly ]                     Ignore label information and just test\n",
      "  --holdout_off                         no holdout data in multiple passes\n",
      "  --holdout_period arg                  holdout period for test only, default \n",
      "                                        10\n",
      "  --holdout_after arg                   holdout after n training examples, \n",
      "                                        default off (disables holdout_period)\n",
      "  --early_terminate arg                 Specify the number of passes tolerated \n",
      "                                        when holdout loss doesn't decrease \n",
      "                                        before early termination, default is 3\n",
      "  --passes arg                          Number of Training Passes\n",
      "  --initial_pass_length arg             initial number of examples per pass\n",
      "  --examples arg                        number of examples to parse\n",
      "  --min_prediction arg                  Smallest prediction to output\n",
      "  --max_prediction arg                  Largest prediction to output\n",
      "  --sort_features                       turn this on to disregard order in \n",
      "                                        which features have been defined. This \n",
      "                                        will lead to smaller cache sizes\n",
      "  --loss_function arg (=squared)        Specify the loss function to be used, \n",
      "                                        uses squared by default. Currently \n",
      "                                        available ones are squared, classic, \n",
      "                                        hinge, logistic and quantile.\n",
      "  --quantile_tau arg (=0.5)             Parameter \\tau associated with Quantile\n",
      "                                        loss. Defaults to 0.5\n",
      "  --l1 arg                              l_1 lambda\n",
      "  --l2 arg                              l_2 lambda\n",
      "  --named_labels arg                    use names for labels (multiclass, etc.)\n",
      "                                        rather than integers, argument \n",
      "                                        specified all possible labels, \n",
      "                                        comma-sep, eg \"--named_labels \n",
      "                                        Noun,Verb,Adj,Punc\"\n",
      "\n",
      "Output model:\n",
      "  -f [ --final_regressor ] arg          Final regressor\n",
      "  --readable_model arg                  Output human-readable final regressor \n",
      "                                        with numeric features\n",
      "  --invert_hash arg                     Output human-readable final regressor \n",
      "                                        with feature names.  Computationally \n",
      "                                        expensive.\n",
      "  --save_resume                         save extra state so learning can be \n",
      "                                        resumed later with new data\n",
      "  --save_per_pass                       Save the model after every pass over \n",
      "                                        data\n",
      "  --output_feature_regularizer_binary arg\n",
      "                                        Per feature regularization output file\n",
      "  --output_feature_regularizer_text arg Per feature regularization output file,\n",
      "                                        in text\n",
      "\n",
      "Output options:\n",
      "  -p [ --predictions ] arg              File to output predictions to\n",
      "  -r [ --raw_predictions ] arg          File to output unnormalized predictions\n",
      "                                        to\n",
      "\n",
      "Reduction options, use [option] --help for more info:\n",
      "\n",
      "  --bootstrap arg                       k-way bootstrap by online importance \n",
      "                                        resampling\n",
      "\n",
      "  --search arg                          Use learning to search, \n",
      "                                        argument=maximum action id or 0 for LDF\n",
      "\n",
      "  --replay_c arg                        use experience replay at a specified \n",
      "                                        level [b=classification/regression, \n",
      "                                        m=multiclass, c=cost sensitive] with \n",
      "                                        specified buffer size\n",
      "\n",
      "  --cbify arg                           Convert multiclass on <k> classes into \n",
      "                                        a contextual bandit problem\n",
      "\n",
      "  --cb_adf                              Do Contextual Bandit learning with \n",
      "                                        multiline action dependent features.\n",
      "\n",
      "  --cb arg                              Use contextual bandit learning with <k>\n",
      "                                        costs\n",
      "\n",
      "  --csoaa_ldf arg                       Use one-against-all multiclass learning\n",
      "                                        with label dependent features.  Specify\n",
      "                                        singleline or multiline.\n",
      "\n",
      "  --wap_ldf arg                         Use weighted all-pairs multiclass \n",
      "                                        learning with label dependent features.\n",
      "                                          Specify singleline or multiline.\n",
      "\n",
      "  --interact arg                        Put weights on feature products from \n",
      "                                        namespaces <n1> and <n2>\n",
      "\n",
      "  --csoaa arg                           One-against-all multiclass with <k> \n",
      "                                        costs\n",
      "\n",
      "  --multilabel_oaa arg                  One-against-all multilabel with <k> \n",
      "                                        labels\n",
      "\n",
      "  --log_multi arg                       Use online tree for multiclass\n",
      "\n",
      "  --ect arg                             Error correcting tournament with <k> \n",
      "                                        labels\n",
      "\n",
      "  --boosting arg                        Online boosting with <N> weak learners\n",
      "\n",
      "  --oaa arg                             One-against-all multiclass with <k> \n",
      "                                        labels\n",
      "\n",
      "  --top arg                             top k recommendation\n",
      "\n",
      "  --replay_m arg                        use experience replay at a specified \n",
      "                                        level [b=classification/regression, \n",
      "                                        m=multiclass, c=cost sensitive] with \n",
      "                                        specified buffer size\n",
      "\n",
      "  --binary                              report loss as binary classification on\n",
      "                                        -1,1\n",
      "\n",
      "  --link arg (=identity)                Specify the link function: identity, \n",
      "                                        logistic or glf1\n",
      "\n",
      "  --stage_poly                          use stagewise polynomial feature \n",
      "                                        learning\n",
      "\n",
      "  --lrqfa arg                           use low rank quadratic features with \n",
      "                                        field aware weights\n",
      "\n",
      "  --lrq arg                             use low rank quadratic features\n",
      "\n",
      "  --autolink arg                        create link function with polynomial d\n",
      "\n",
      "  --new_mf arg                          rank for reduction-based matrix \n",
      "                                        factorization\n",
      "\n",
      "  --nn arg                              Sigmoidal feedforward network with <k> \n",
      "                                        hidden units\n",
      "\n",
      "  --confidence                          Get confidence for binary predictions\n",
      "\n",
      "  --active_cover                        enable active learning with cover\n",
      "\n",
      "  --active                              enable active learning\n",
      "\n",
      "  --replay_b arg                        use experience replay at a specified \n",
      "                                        level [b=classification/regression, \n",
      "                                        m=multiclass, c=cost sensitive] with \n",
      "                                        specified buffer size\n",
      "\n",
      "  --bfgs                                use bfgs optimization\n",
      "\n",
      "  --conjugate_gradient                  use conjugate gradient based \n",
      "                                        optimization\n",
      "\n",
      "  --lda arg                             Run lda with <int> topics\n",
      "\n",
      "  --noop                                do no learning\n",
      "\n",
      "  --print                               print examples\n",
      "\n",
      "  --rank arg                            rank for matrix factorization.\n",
      "\n",
      "  --sendto arg                          send examples to <host>\n",
      "\n",
      "  --svrg                                Streaming Stochastic Variance Reduced \n",
      "                                        Gradient\n",
      "\n",
      "  --ftrl                                FTRL: Follow the Proximal Regularized \n",
      "                                        Leader\n",
      "\n",
      "  --pistol                              FTRL: Parameter-free Stochastic \n",
      "                                        Learning\n",
      "\n",
      "  --ksvm                                kernel svm\n",
      "\n",
      "Gradient Descent options:\n",
      "  --sgd                                 use regular stochastic gradient descent\n",
      "                                        update.\n",
      "  --adaptive                            use adaptive, individual learning \n",
      "                                        rates.\n",
      "  --invariant                           use safe/importance aware updates.\n",
      "  --normalized                          use per feature normalized updates\n",
      "  --sparse_l2 arg (=0)                  use per feature normalized updates\n",
      "\n",
      "Input options:\n",
      "  -d [ --data ] arg                     Example Set\n",
      "  --daemon                              persistent daemon mode on port 26542\n",
      "  --port arg                            port to listen on; use 0 to pick unused\n",
      "                                        port\n",
      "  --num_children arg                    number of children for persistent \n",
      "                                        daemon mode\n",
      "  --pid_file arg                        Write pid file in persistent daemon \n",
      "                                        mode\n",
      "  --port_file arg                       Write port used in persistent daemon \n",
      "                                        mode\n",
      "  -c [ --cache ]                        Use a cache.  The default is \n",
      "                                        <data>.cache\n",
      "  --cache_file arg                      The location(s) of cache_file.\n",
      "  -k [ --kill_cache ]                   do not reuse existing cache: create a \n",
      "                                        new one always\n",
      "  --compressed                          use gzip format whenever possible. If a\n",
      "                                        cache file is being created, this \n",
      "                                        option creates a compressed cache file.\n",
      "                                        A mixture of raw-text & compressed \n",
      "                                        inputs are supported with \n",
      "                                        autodetection.\n",
      "  --no_stdin                            do not default to reading from stdin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!vw --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/truename/Documents/capstone_user_identification/vw_cache.csv\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  unknown   0.0000        1\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1\n",
      "passes used = 1\n",
      "weighted example sum = 1.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.000000\n",
      "total feature number = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = /home/truename/Documents/capstone_user_identification/vw_cache.csv\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 0\n",
      "passes used = 1\n",
      "weighted example sum = 0\n",
      "weighted label sum = 0\n",
      "average loss = -nan\n",
      "total feature number = 0\n",
      "final_regressor = /home/truename/Documents/capstone_user_identification/vw_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = /home/truename/Documents/capstone_user_identification/train_part.vw.cache\n",
      "Reading datafile = /home/truename/Documents/capstone_user_identification/train_part.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      262        1       11\n",
      "1.000000 1.000000            2            2.0       82      262       11\n",
      "1.000000 1.000000            4            4.0      241      262       11\n",
      "1.000000 1.000000            8            8.0      352      262       11\n",
      "1.000000 1.000000           16           16.0      135       16       11\n",
      "1.000000 1.000000           32           32.0       71      112       11\n",
      "0.968750 0.937500           64           64.0      358      231       11\n",
      "0.976562 0.984375          128          128.0      348      346       11\n",
      "0.941406 0.906250          256          256.0      202      202       11\n",
      "0.947266 0.953125          512          512.0       30        1       11\n",
      "0.925781 0.904297         1024         1024.0       36      290       11\n",
      "0.908203 0.890625         2048         2048.0       21      128       11\n",
      "0.880127 0.852051         4096         4096.0       80      229       11\n",
      "0.856323 0.832520         8192         8192.0      307      356       11\n",
      "0.828003 0.799683        16384        16384.0       59      193       11\n",
      "0.795441 0.762878        32768        32768.0      262       30       11\n",
      "0.760468 0.725494        65536        65536.0      171      238       11\n",
      "0.724008 0.724008       131072       131072.0        6        6       11 h\n",
      "0.697339 0.670672       262144       262144.0       12       12       11 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 115160\n",
      "passes used = 3\n",
      "weighted example sum = 345480.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.661352 h\n",
      "total feature number = 3800280\n",
      "CPU times: user 300 ms, sys: 56 ms, total: 356 ms\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -k $PATH_TO_DATA'/vw_cache.csv'\n",
    "!vw --cache_file $PATH_TO_DATA'/vw_cache.csv'\n",
    "!vw --oaa 400 $train_part_vw  --passes 3 -c -b 26 --random_seed 17 -f $model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите прогнозы на выборке *valid.vw* в *vw_valid_pred.csv*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = /home/truename/Documents/capstone_user_identification/vw_valid_pred.txt\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/truename/Documents/capstone_user_identification/valid.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        4      188       11\n",
      "1.000000 1.000000            2            2.0      160      220       11\n",
      "0.750000 0.500000            4            4.0      143      143       11\n",
      "0.750000 0.750000            8            8.0      247      247       11\n",
      "0.687500 0.625000           16           16.0      341       30       11\n",
      "0.593750 0.500000           32           32.0      237      237       11\n",
      "0.609375 0.625000           64           64.0      178      178       11\n",
      "0.640625 0.671875          128          128.0      132      228       11\n",
      "0.656250 0.671875          256          256.0       14       14       11\n",
      "0.646484 0.636719          512          512.0      370      370       11\n",
      "0.663086 0.679688         1024         1024.0      189      189       11\n",
      "0.655762 0.648438         2048         2048.0      311      311       11\n",
      "0.657227 0.658691         4096         4096.0      195      318       11\n",
      "0.660156 0.663086         8192         8192.0      171      195       11\n",
      "0.657654 0.655151        16384        16384.0      362       51       11\n",
      "0.655121 0.652588        32768        32768.0      248      248       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 54838\n",
      "passes used = 1\n",
      "weighted example sum = 54838.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.654583\n",
      "total feature number = 603218\n",
      "CPU times: user 28 ms, sys: 12 ms, total: 40 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i $model -t -d $valid_vw -p $PATH_TO_DATA'/vw_valid_pred.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считайте прогнозы *kaggle_data/vw_valid_pred.csv*  из файла и посмотрите на долю правильных ответов на отложенной части.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34541741128414605"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = pd.read_csv(os.path.join(PATH_TO_DATA,'vw_valid_pred.csv'),header=-1)\n",
    "test_prediction = [float(x[0]) for x in test_prediction.values]\n",
    "auc = accuracy_score(y_valid_for_vw, test_prediction)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Теперь обучите `SGDClassifier` (3 прохода по выборке, логистическая функция потерь) и `LogisticRegression` на 70% разреженной обучающей выборки – `(X_train_part_sparse, y_train_part)`, сделайте прогноз для отложенной выборки `(X_valid_sparse, y_valid)` и посчитайте доли верных ответов. Логистическая регрессия будет обучаться не быстро (у меня – 4 минуты) – это нормально. Укажите везде `random_state`=17, `n_jobs`=-1. Для `SGDClassifier` также укажите `max_iter=3`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "logit = LogisticRegression(random_state=17, n_jobs=-1)\n",
    "sgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17, max_iter=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.365\n",
      "CPU times: user 26min 46s, sys: 260 ms, total: 26min 46s\n",
      "Wall time: 26min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_part_sparse,y_train_part)\n",
    "logit_y_predicted = logit.predict(X_valid_sparse)\n",
    "logit_accuracy = accuracy_score(y_valid, logit_y_predicted)\n",
    "print(\"Accuracy train: %0.3f\" % logit_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.286\n",
      "CPU times: user 1min 10s, sys: 392 ms, total: 1min 10s\n",
      "Wall time: 9.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_part_sparse,y_train_part)\n",
    "sgd_y_predicted = sgd_logit.predict(X_valid_sparse)\n",
    "sgd_logit_accuracy = accuracy_score(y_valid, sgd_y_predicted)\n",
    "print(\"Accuracy train: %0.3f\" % sgd_logit_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Вопрос 1. </font> Посчитайте долю правильных ответов на отложенной выборке для Vowpal Wabbit, округлите до 3 знаков после запятой.**\n",
    "\n",
    "**<font color='red'>Вопрос 2. </font> Посчитайте долю правильных ответов на отложенной выборке для SGD, округлите до 3 знаков после запятой.**\n",
    "\n",
    "**<font color='red'>Вопрос 3. </font> Посчитайте долю правильных ответов на отложенной выборке для логистической регрессии, округлите до 3 знаков после запятой.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_valid_acc = 0.345\n",
    "sgd_valid_acc = 0.286\n",
    "logit_valid_acc = 0.365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_answer_to_file(answer, file_address):\n",
    "    with open(file_address, 'w') as out_f:\n",
    "        out_f.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_answer_to_file(round(vw_valid_acc, 3), 'answer6_1.txt')\n",
    "write_answer_to_file(round(sgd_valid_acc, 3), 'answer6_2.txt')\n",
    "write_answer_to_file(round(logit_valid_acc, 3), 'answer6_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Валидация по тестовой выборке (Public Leaderboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучите модель VW с теми же параметрами на всей обучающей выборке – *train.vw*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/truename/Documents/capstone_user_identification/vw_cache.csv\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0  unknown   0.0000        1\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 1\n",
      "passes used = 1\n",
      "weighted example sum = 1.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.000000\n",
      "total feature number = 1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using cache_file = /home/truename/Documents/capstone_user_identification/vw_cache.csv\n",
      "ignoring text input in favor of cache input\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 0\n",
      "passes used = 1\n",
      "weighted example sum = 0\n",
      "weighted label sum = 0\n",
      "average loss = -nan\n",
      "total feature number = 0\n",
      "final_regressor = /home/truename/Documents/capstone_user_identification/vw_model.vw\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = /home/truename/Documents/capstone_user_identification/train.vw.cache\n",
      "Reading datafile = /home/truename/Documents/capstone_user_identification/train.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0      262        1       11\n",
      "1.000000 1.000000            2            2.0       82      262       11\n",
      "1.000000 1.000000            4            4.0      241      262       11\n",
      "1.000000 1.000000            8            8.0      352      262       11\n",
      "1.000000 1.000000           16           16.0      135       16       11\n",
      "1.000000 1.000000           32           32.0       71      112       11\n",
      "0.968750 0.937500           64           64.0      358      231       11\n",
      "0.976562 0.984375          128          128.0      348      346       11\n",
      "0.941406 0.906250          256          256.0      202      202       11\n",
      "0.947266 0.953125          512          512.0       30        1       11\n",
      "0.925781 0.904297         1024         1024.0       36      290       11\n",
      "0.908203 0.890625         2048         2048.0       21      128       11\n",
      "0.880127 0.852051         4096         4096.0       80      229       11\n",
      "0.856323 0.832520         8192         8192.0      307      356       11\n",
      "0.828003 0.799683        16384        16384.0       59      193       11\n",
      "0.795441 0.762878        32768        32768.0      262       30       11\n",
      "0.760468 0.725494        65536        65536.0      171      238       11\n",
      "0.725319 0.690170       131072       131072.0      180      159       11\n",
      "0.692989 0.692989       262144       262144.0       88      221       11 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 164514\n",
      "passes used = 3\n",
      "weighted example sum = 493542.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.642595 h\n",
      "total feature number = 5428962\n",
      "CPU times: user 352 ms, sys: 144 ms, total: 496 ms\n",
      "Wall time: 17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -k $PATH_TO_DATA'/vw_cache.csv'\n",
    "!vw --cache_file $PATH_TO_DATA'/vw_cache.csv'\n",
    "!vw --oaa 400 $train_vw  --passes 3 -c -b 26 --random_seed 17 -f $model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз для тестовой выборки.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = /home/truename/Documents/capstone_user_identification/vw_test_pred.txt\n",
      "Num weight bits = 26\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = /home/truename/Documents/capstone_user_identification/test.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        1       90       11\n",
      "1.000000 1.000000            2            2.0        1       21       11\n",
      "1.000000 1.000000            4            4.0        1      265       11\n",
      "1.000000 1.000000            8            8.0        1      137       11\n",
      "1.000000 1.000000           16           16.0        1      273       11\n",
      "1.000000 1.000000           32           32.0        1      384       11\n",
      "1.000000 1.000000           64           64.0        1      139       11\n",
      "1.000000 1.000000          128          128.0        1       85       11\n",
      "1.000000 1.000000          256          256.0        1       25       11\n",
      "0.994141 0.988281          512          512.0        1      364       11\n",
      "0.990234 0.986328         1024         1024.0        1      202       11\n",
      "0.992188 0.994141         2048         2048.0        1      181       11\n",
      "0.993652 0.995117         4096         4096.0        1       21       11\n",
      "0.994629 0.995605         8192         8192.0        1      137       11\n",
      "0.995300 0.995972        16384        16384.0        1      326       11\n",
      "0.994568 0.993835        32768        32768.0        1       10       11\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 46473\n",
      "passes used = 1\n",
      "weighted example sum = 46473.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.994642\n",
      "total feature number = 511203\n",
      "CPU times: user 24 ms, sys: 16 ms, total: 40 ms\n",
      "Wall time: 892 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!vw -i $model -t -d $test_vw -p $PATH_TO_DATA'/vw_test_pred.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите прогноз в файл, примените обратное преобразование меток (был LabelEncoder и потом +1 в меткам) и отправьте решение на Kaggle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label,float_format='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = pd.read_csv(os.path.join(PATH_TO_DATA,'vw_test_pred.txt'),header=-1,names=['user_id'])\n",
    "#vw_pred = test_prediction.astype('int') #[float(x[0]) for x in test_prediction.values]\n",
    "vw_pred = class_encoder.inverse_transform(test_prediction.astype('int') - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(vw_pred, os.path.join(PATH_TO_DATA, 'vw_400_users.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте то же самое для SGD и логистической регрессии. Тут уже ждать обучение логистической регрессии совсем скучно (заново запускать тетрадку вам не захочется), но давайте дождемся.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1228: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = -1.\n",
      "  \" = {}.\".format(self.n_jobs))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 51s, sys: 360 ms, total: 43min 52s\n",
      "Wall time: 42min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit.fit(X_train_sparse,y)\n",
    "sgd_logit.fit(X_train_sparse,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_test_pred = logit.predict(X_test_sparse)\n",
    "sgd_logit_test_pred = sgd_logit.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred, \n",
    "                         os.path.join(PATH_TO_DATA, 'logit_400_users.csv'))\n",
    "write_to_submission_file(sgd_logit_test_pred,\n",
    "                         os.path.join(PATH_TO_DATA, 'sgd_400_users.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на доли правильных ответов на публичной части (public leaderboard) тестовой выборки [этого](https://inclass.kaggle.com/c/identify-me-if-you-can4) соревнования.\n",
    "\n",
    "**<font color='red'>Вопрос 4. </font> Какова доля правильных ответов на публичной части тестовой выборки (public leaderboard)  для Vowpal Wabbit?**\n",
    "\n",
    "**<font color='red'>Вопрос 5. </font> Какова доля правильных ответов на публичной части тестовой выборки (public leaderboard)  для SGD?**\n",
    "\n",
    "**<font color='red'>Вопрос 6. </font> Какова доля правильных ответов на публичной части тестовой выборки (public leaderboard)  для логистической регрессии?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vw_lb_score, sgd_lb_score, logit_lb_score = 0.188, 0.174, 0.2\n",
    "\n",
    "write_answer_to_file(round(vw_lb_score, 3), 'answer6_4.txt')\n",
    "write_answer_to_file(round(sgd_lb_score, 3), 'answer6_5.txt')\n",
    "write_answer_to_file(round(logit_lb_score, 3), 'answer6_6.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В заключение по заданию:**\n",
    "- Про соотношение качества классификации и скорости обучения VW, SGD и logit выводы предлагается сделать самостоятельно\n",
    "- Пожалуй, задача классификации на 400 классов (идентификация 400 пользователей) решается недостаточно хорошо при честном отделении по времени тестовой выборки от обучающей. Далее мы будем соревноваться в идентификации одного пользователя (Элис) – [вот](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) соревнование, в котором предлагается поучаствовать. Не перепутайте! \n",
    "\n",
    "**Удачи!**"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
