{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time, datetime\n",
    "import random\n",
    "import sklearn.metrics\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные из Excel-файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# задаём путь к папке, где лежат данные\n",
    "folder_name = 'D:\\\\Users\\\\AATimchenko\\\\Documents\\\\Doc-Projects\\\\RDF\\\\Моделирование расчета TS\\\\NeuralNetworkForecast\\\\'\n",
    "\n",
    "# определяем список столбцов c с параметрами, которые оставляем для использования в прогнозе\n",
    "cols_to_retain = ['Подкатегория','Товарная подгруппа','IntakeWeek','ExitWeek','NSize','Пол','ТМ','Возраст 1','Товар драйвер']\n",
    "\n",
    "# эти колонки для прогноза не нужны, но мы их оставляем, т.к. будем их использовать для анализа точности прогнозов\n",
    "id_cols = ['Цветомодель', 'Наименование']\n",
    "\n",
    "# задаем количество прошлых недель, по которым будем строить прогноз\n",
    "#weeks = ['w6','w5','w4','w3','w2','w1']\n",
    "#query_not_zero_weeks = 'w6 > 0 or w5 > 0 or w4 > 0 or w3 > 0 or w2 > 0 or w1 > 0'\n",
    "w = 6\n",
    "weeks = ['w'+str(i) for i in range (w,0,-1)]\n",
    "query_not_zero_weeks = (' > 0 or ').join(weeks) + ' > 0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем данные по IntakeDate и ExitDate из Excel-файла (данные брал из ПРК)\n",
    "file_name_data = folder_name + 'IntakeDates.xlsx'\n",
    "df_dates = pd.read_excel(io = file_name_data)\n",
    "df_dates_cols = ['ID','NSize','IntakeDate','ExitDate']\n",
    "df_dates = df_dates [df_dates_cols]\n",
    "\n",
    "print(df_dates.shape)\n",
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# функция для преобразования данных о продажах из Excel-файлов, выгруженных из ТМА и ПРК, в нужный формат\n",
    "def preprocess_data (data, df_dates):\n",
    "\n",
    "    cols_to_read = ['Цветомодель',\n",
    "                    'Наименование',\n",
    "                    'Группа товаров',\n",
    "                    'Подкатегория',\n",
    "                    'Товарная группа',\n",
    "                    'Товарная подгруппа',\n",
    "                    'Пол',\n",
    "                    'Возраст 1',\n",
    "                    'ТМ',\n",
    "                    'Коллекция',\n",
    "                    'Тип товара',\n",
    "                    'Товар драйвер',\n",
    "                    'Менеджер']\n",
    "    \n",
    "    for x in data.columns:\n",
    "        if x not in cols_to_read and type(x) != datetime.datetime: \n",
    "            data.drop (x, axis = 1, inplace = True)\n",
    "\n",
    "    data.dropna(axis=0, how='any', thresh=None, subset=['Группа товаров'], inplace=True)\n",
    "    data = data.assign(ID = data['Цветомодель'] + '=' + data['Коллекция'])\n",
    "    \n",
    "    data = data.set_index('ID').join(df_dates.set_index('ID'))\n",
    "    \n",
    "    data.dropna(axis=0, how='any', thresh=None, subset=['IntakeDate','ExitDate'], inplace=True)\n",
    "    data['Товар драйвер'].fillna(value = 'Нет', inplace = True)\n",
    "    \n",
    "    # считаем и добавляем столбцы IntakeWeek и ExitWeek - плановая неделя начала продаж\n",
    "    data = data.assign(IntakeWeek = lambda x: [x.isocalendar()[1] for x in data['IntakeDate']])\n",
    "    data = data.assign(ExitWeek = lambda x: [x.isocalendar()[1] for x in data['ExitDate']])\n",
    "\n",
    "    # получаем список столбцов, в которые данные о продажах (у них в шапках дата)\n",
    "    week_cols = [x for x in data.columns if type(x) == datetime.datetime]\n",
    "   \n",
    "    new_data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0,len(week_cols)-len(weeks)):\n",
    "        \n",
    "        # берем w+1 столбцов с продажами\n",
    "        add = data[week_cols].iloc[:,i:i+len(weeks)+1]\n",
    "\n",
    "        # переименовываем столбцы\n",
    "        #add.columns= ['w4','w3','w2','w1','w0']\n",
    "        add.columns= weeks + ['w0']\n",
    "\n",
    "        # добавляем столбец с номером последней недели\n",
    "        add.loc [:,'WeekNumber']= np.array([data[week_cols].columns[i+len(weeks)].isocalendar()[1]]*len(add))\n",
    "\n",
    "        # добавляем атрибуты товара и добавляем в целевую таблицу\n",
    "        new_data = new_data.append(add.merge(data[id_cols+cols_to_retain], left_index = True, right_index = True), ignore_index=True)\n",
    "\n",
    "    #удаляем наборы данных, где продажи во всех прошлых неделях нулевые\n",
    "    new_data = new_data.query(query_not_zero_weeks)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем понедельные данные о продажах из excel\n",
    "file_name_data = folder_name + 'AppDataForNN*'\n",
    "data = pd.DataFrame()\n",
    "\n",
    "for f in glob.glob(file_name_data):\n",
    "    print (f)\n",
    "    df = pd.read_excel(io = f, sheetname = 'Продажи-ШТ')\n",
    "    data = data.append(preprocess_data (df,df_dates))\n",
    "    \n",
    "# Сохраняем считанное в файл, нужно для отладки чтобы потом много раз не читать excel\n",
    "joblib.dump(data, 'data.pkl')\n",
    "\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выгрузка подготовленных данных в файл для теста\n",
    "#file_test = folder_name + 'TestData.xlsx'\n",
    "#data.to_excel(excel_writer=file_test, sheet_name='Sheet1',startrow=0, startcol=0)\n",
    "# ==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразовываем загруженные данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем исходные данные из дампа\n",
    "#data = joblib.load('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставляем только нужные подкатегории\n",
    "#data = data.query('Подкатегория == \"Спортивный стиль\" or Подкатегория == \"Basic Sport\" or Подкатегория == \"Outdoor Core\" or Подкатегория == \"Outdoor Style\"')\n",
    "data = data.query('Подкатегория == \"Outdoor Core\" or Подкатегория == \"Outdoor Style\"')\n",
    "\n",
    "#сохраняем отфильтрованные данные, далее к ним допишем прогноз\n",
    "filtered_data = data\n",
    "joblib.dump(filtered_data, 'filtered_data.pkl')\n",
    "\n",
    "# удаляем столбцы с идентификаторами товара (Цветомодель и Наименование)\n",
    "train = filtered_data.drop (id_cols, axis = 1)\n",
    "#train.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция преобразует текстовые параметры в числовые столбцы, с которыми работает прогнозная модель\n",
    "def transform_data (data = None, vectorizer = None, max_data = None):\n",
    "    \n",
    "    # numeric x - создаем матрицу с численными параметрами\n",
    "    numeric_cols = weeks + ['NSize']\n",
    "    x_num_data = data[numeric_cols].as_matrix()\n",
    "\n",
    "    # scale to <0,1> - нормируем численные параметры от 0 до 1\n",
    "    if max_data is None: max_data = np.amax( x_num_data, 0 )\n",
    "    x_num_data = x_num_data / max_data\n",
    "\n",
    "    # y\n",
    "    y_data = data.w0\n",
    "\n",
    "    # categorical - берем только категорийные параметры\n",
    "    cat_data = data.drop( numeric_cols + ['w0'] , axis = 1 )\n",
    "    cat_data.fillna( 'NA', inplace = True )\n",
    "\n",
    "    x_cat_data = cat_data.to_dict( orient = 'records' )\n",
    "\n",
    "    # vectorize - векторизуем категорийные параметры\n",
    "    if vectorizer is None :\n",
    "        vectorizer = DV( sparse = False )\n",
    "        vec_x_cat_data = vectorizer.fit_transform( x_cat_data )\n",
    "    else:\n",
    "        vec_x_cat_data = vectorizer.transform( x_cat_data )\n",
    "\n",
    "    # complete x - соединяем вместе численные столбцы и векторизированные категорийные столбцы\n",
    "    x_data = np.hstack(( x_num_data, vec_x_cat_data ))\n",
    "    \n",
    "    return x_data, y_data, max_data, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X - матрица для обучения, y - вектор правильных ответов\n",
    "X, y, max_data, vectorizer = transform_data (data = train, vectorizer = None, max_data = None)\n",
    "\n",
    "# сохраняем промежуточные результаты в файлы\n",
    "joblib.dump(X, 'x.pkl')\n",
    "joblib.dump(y, 'y.pkl')\n",
    "joblib.dump(max_data, 'max_data.pkl')\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "\n",
    "# размерность матрицы входящих данных для сети\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем сеть, параметры выбираем вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фиксируем генератор случайных чисел, нужно для повторяемости результатов\n",
    "seed = 24\n",
    "np.random.seed(seed)\n",
    "\n",
    "# задаем параметры сети вручную\n",
    "# число нейронов в скрытых слоях делаю равным удвоенному количеству входных нейронов (потом нужно подбирать)\n",
    "# n = X.shape[1]*2\n",
    "\n",
    "# число слоев и нейронов\n",
    "hls = ( X.shape[1]*2, X.shape[1]*2, ) # показала хороший результат R2 = 0.82\n",
    "# hls = (100,100,)                   # показала хороший результат R2 = 0.81\n",
    "\n",
    "\n",
    "# задаем модель\n",
    "MLP = MLPRegressor (\n",
    "                    activation = 'logistic',       # функция активации \n",
    "                    hidden_layer_sizes = hls, \n",
    "                    alpha = 0.0001,                # регуляризация default = 0.0001\n",
    "                    warm_start = False, \n",
    "                    verbose = True, \n",
    "                    tol = 0.00001)                 # Tolerance for the optimization default = 0.0001\n",
    "\n",
    "# делим данные на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# обучаем сеть на тестовых данных, замеряем время\n",
    "MLP_start = time.perf_counter()\n",
    "MLP.fit (X_train, y_train)\n",
    "MLP_end = time.perf_counter()\n",
    "\n",
    "# сохраняем обученную сеть в файл\n",
    "joblib.dump(MLP, 'mlp.pkl')\n",
    "\n",
    "# смотрим результаты\n",
    "print('=============================')\n",
    "print(MLP.get_params())\n",
    "print('=============================')\n",
    "print('Time to fit: ',(MLP_end - MLP_start))\n",
    "print('In-Sample R2_score: ', MLP.score(X_train, y_train))\n",
    "print('Out-of-Sample R2_score: ', MLP.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# расчёт прогноза по тестовой выборке Out-of-Sample\n",
    "MLP_start = time.perf_counter()\n",
    "forecast = MLP.predict(X_test)\n",
    "MLP_end = time.perf_counter()\n",
    "\n",
    "print('=============================')\n",
    "print(\"Time to predict Out-of-Sample: {0}\".format(MLP_end - MLP_start))\n",
    "print(\"Out-of-Sample R2_score: {0}\".format(sklearn.metrics.r2_score(y_true=y_test, y_pred=forecast)))\n",
    "print(\"mean_absolute_error: {0}\".format(sklearn.metrics.mean_absolute_error(y_true=y_test, y_pred=forecast)))\n",
    "print(\"mean_squared_error: {0}\".format(sklearn.metrics.mean_squared_error(y_true=y_test, y_pred=forecast)))\n",
    "print(\"median_absolute_error: {0}\".format(sklearn.metrics.median_absolute_error(y_true=y_test, y_pred=forecast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если результаты ОК, то можно переобучить сеть по всем имеющимся данным\n",
    "MLP_start = time.perf_counter()\n",
    "MLP.fit(X, y)\n",
    "MLP_end = time.perf_counter()\n",
    "\n",
    "# сохраняем обученную сеть в файл\n",
    "joblib.dump(MLP, 'mlp.pkl')\n",
    "\n",
    "# смотрим результаты\n",
    "print('Time: ', (MLP_end - MLP_start))\n",
    "print('All data R2_score: ',MLP.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Расчет прогноза и запись результатов в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если исходные данные для прогноза нужно загрузить из Excel\n",
    "#file_test = folder_name + 'TestData.xlsx'\n",
    "#test = pd.read_excel(io = file_name_data)\n",
    "\n",
    "# исходные данные\n",
    "test = filtered_data\n",
    "\n",
    "# Оставляем только нужные столбцы\n",
    "test = test[weeks + ['w0','WeekNumber'] + cols_to_retain]\n",
    "\n",
    "# преобразовываем данные к нужному виду\n",
    "X_test, y_test, vectorizer, max_data = transform_data (data = test, vectorizer = vectorizer, max_data = max_data)\n",
    "\n",
    "#расчёт прогноза\n",
    "MLP_start = time.perf_counter()\n",
    "forecast = MLP.predict(X_test)\n",
    "MLP_end = time.perf_counter()\n",
    "\n",
    "print(\"Time: {0}\".format(MLP_end - MLP_start))\n",
    "print(\"R2_score: {0}\".format(sklearn.metrics.r2_score(y_true=y_test, y_pred=forecast)))\n",
    "print(\"mean_absolute_error: {0}\".format(sklearn.metrics.mean_absolute_error(y_true=y_test, y_pred=forecast)))\n",
    "print(\"mean_squared_error: {0}\".format(sklearn.metrics.mean_squared_error(y_true=y_test, y_pred=forecast)))\n",
    "print(\"median_absolute_error: {0}\".format(sklearn.metrics.median_absolute_error(y_true=y_test, y_pred=forecast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# исходные данные с идентификационными столбцами\n",
    "result = filtered_data\n",
    "\n",
    "# добавляем к исходным данным расчитанный прогноз и считаем ошибки прогнозы\n",
    "result = result.assign(forecast = forecast)\n",
    "result = result.assign(error = result.w0 - result.forecast)\n",
    "result = result.assign(abs_error = abs(result.error))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# записываем результат в файл\n",
    "file_result = folder_name + 'Forecast.xlsx'\n",
    "result.to_excel(excel_writer=file_result, sheet_name='Sheet1',startrow=0, startcol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готово! Можно проверять прогнозы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример R2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объяснение на примере, что показывает меткира R2-score\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_true = [0.1, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0]\n",
    "y_pred = [0.9, 0.1, 1.7, 1.0, 0.1, 0.1, 2.5, 1.5]\n",
    "\n",
    "r2_score (y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучаем сеть с перебором параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Загружаем данные из дампов\n",
    "X = joblib.load('x.pkl')\n",
    "y = joblib.load('y.pkl')\n",
    "max_train = joblib.load('max_train.pkl')\n",
    "vectorizer = joblib.load('vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix random seed for reproducibility\n",
    "seed = 24\n",
    "np.random.seed(seed)\n",
    "\n",
    "# разбиваем данные на обучающую и тестовую выборку\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "## General and CV options\n",
    "n_iter = 10\n",
    "n_jobs = -1\n",
    "cv = 5\n",
    "max_iter = 200\n",
    "batch_size = 'auto'\n",
    "\n",
    "## Parameters for MLP\n",
    "neurons_output = max(1, y_train.shape[1] if len(y_train.shape) > 1 else 1)\n",
    "sumInOutSize = X_train.shape[1] + neurons_output\n",
    "maxNeuron = sumInOutSize\n",
    "minNeuron = int(maxNeuron/2)\n",
    "input_dim = X_train.shape[1] if len(X_train.shape) > 1 else 1\n",
    "output_dim = y_train.shape[1] if len(y_train.shape) > 1 else 1\n",
    "n_layers = 1\n",
    "\n",
    "## MLPRegressor\n",
    "MLPR_mdl = MLPRegressor(random_state=seed, max_iter=max_iter, batch_size=batch_size)\n",
    "\n",
    "# задаем параметры сеты, которые будем перебирать\n",
    "MLPR_param_distributions = \\\n",
    "    { \n",
    "#        \"hidden_layer_sizes\": [(x,) for x in range(minNeuron,maxNeuron)],\n",
    "        \"hidden_layer_sizes\": list(hls),\n",
    "        \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "        \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "        \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "        \"alpha\": list(np.random.uniform(low=0, high=5, size=n_iter)),\n",
    "        \"learning_rate_init\": list(10.0 ** np.random.uniform(low=-6, high=-2, size=n_iter)),\n",
    "        \"momentum\": list(np.random.uniform(low=1E-2, high=1, size=n_iter)),\n",
    "        \"nesterovs_momentum\": [True, False],\n",
    "        \"beta_1\": list(np.random.uniform(low=1E-2, high=1, size=n_iter)),\n",
    "        \"beta_2\": list(np.random.uniform(low=1E-2, high=1, size=n_iter))\n",
    "    }\n",
    "    \n",
    "#param_grid = ParameterGrid(MLPR_param_distributions)\n",
    "#grid_size = len(param_grid)\n",
    "#print(grid_size)\n",
    "\n",
    "MLPR_random_search = RandomizedSearchCV(estimator=MLPR_mdl, cv=cv, n_iter=n_iter, param_distributions=MLPR_param_distributions, n_jobs=n_jobs)\n",
    "MLPR_start = time.perf_counter()\n",
    "MLPR_result = MLPR_random_search.fit(X_train, y_train)\n",
    "MLPR_end = time.perf_counter()\n",
    "MLPR_y_pred = MLPR_result.predict(X=X_test)\n",
    "\n",
    "print('Train MLPR_result.score: ',MLPR_result.score(X_train, y_train))\n",
    "print('Test MLPR_result.score: ',MLPR_result.score(X_test, y_test))\n",
    "\n",
    "## Summarize results\n",
    "print(\"MLPRegressor\")\n",
    "print(\"============\")\n",
    "print(\"Time: {0}\".format(MLPR_end - MLPR_start))\n",
    "print(\"Score: {0}\".format(MLPR_result.best_score_))\n",
    "print(\"Parameters: {0}\".format(MLPR_result.best_params_))\n",
    "print(\"r2_score: {0}\".format(sklearn.metrics.r2_score(y_true=y_test, y_pred=MLPR_y_pred)))\n",
    "print(\"mean_absolute_error: {0}\".format(sklearn.metrics.mean_absolute_error(y_true=y_test, y_pred=MLPR_y_pred)))\n",
    "print(\"mean_squared_error: {0}\".format(sklearn.metrics.mean_squared_error(y_true=y_test, y_pred=MLPR_y_pred)))\n",
    "print(\"median_absolute_error: {0}\".format(sklearn.metrics.median_absolute_error(y_true=y_test, y_pred=MLPR_y_pred)))\n",
    "#print(\"mean_absolute_percentage_error: {0}\".format(mean_absolute_percentage_error(y_true=y_test, y_pred=MLPR_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
