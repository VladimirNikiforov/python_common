{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vladimir Nikiforov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 0 - PREPARE ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ht_pg_server\n",
      "620c864dc1b5\n",
      "Untagged: postgres:11\n",
      "Untagged: postgres@sha256:68b49a280d2fbe9330c0031970ebb72015e1272dfa25f0ed7557514f9e5ad7b7\n",
      "Deleted: sha256:53912975086f1470f877922e3eb79c4f17c403fcb0b51b1f45df9b623987600e\n",
      "Deleted: sha256:859135cc89a4161becb56082dc95772d26d157bc67622be28ccab159cee9c520\n",
      "Deleted: sha256:85d5ba072f703aed8ed136ee24ef0a5c22fb818e307db40be0637df1691546fe\n",
      "Deleted: sha256:44bc37a16317acf120ebcb0b8aa2cf8738f0dcc40ca47f18d9d5789099089cd0\n",
      "Deleted: sha256:050e2fbc4c511fff109624929dcc24d61f4565052ffd6e30e3d46d42ce65c576\n",
      "Deleted: sha256:6e7901ce413d382c6544cf07b5370341507a4734525947ecef28944c041e39b6\n",
      "Deleted: sha256:c0d5a0e28851bc9a301c0cb92e03c036c1a4b0d8da1efc4a16a921abdd641e3e\n",
      "Deleted: sha256:21cf04dd5b4e7ddbe4a7f122b532038c351eef9057ff71940a1a0a36069fabd5\n",
      "Deleted: sha256:5eba02d7eb8edfd2d0c4503cd6976cfb526e96c4f62960bac5efdf86a1e71e4f\n",
      "Deleted: sha256:0e0ff67afa7921baa6b3c7afd83664752663b5b8704c5236941359003e6f8887\n",
      "Deleted: sha256:915e9e2c13848dc5933cea4c23b53649971b434b1dc1a8ca208f3f47f86ceab1\n",
      "Deleted: sha256:b36245f574fc056b60e016e9db27440a7daac357cf2cc1eb2407a791b01d5ea0\n",
      "Deleted: sha256:33caa0d1d5a95acf62a0767e830c2f656d652771edd7b8c7b04bf97af98ba40f\n",
      "Deleted: sha256:c2c9479f4b920b4454d1b077424280197a3ee03a4c157fec41427af7683dce95\n",
      "Deleted: sha256:d56055da3352f918f4d8a42350385ea5b10d0906e746a8fbb4b850f9284deee5\n"
     ]
    }
   ],
   "source": [
    "# Stop previously runned server\n",
    "!docker stop ht_pg_server\n",
    "!docker rm $(docker ps -a -q)\n",
    "!docker rmi $(docker images -q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'postgres:11' locally\n",
      "11: Pulling from library/postgres\n",
      "\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[2B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[3B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[4B\n",
      "\u001b[1B\n",
      "\u001b[1BDigest: sha256:68b49a280d2fbe9330c0031970ebb72015e1272dfa25f0ed7557514f9e5ad7b7\n",
      "Status: Downloaded newer image for postgres:11\n",
      "417598d11fbee5a7da5a037dcd217abc78c507bcefe307fe729ae10d2f3a9aaf\n"
     ]
    }
   ],
   "source": [
    "# Run the server:\n",
    "!docker run -d --name ht_pg_server -v ht_dbdata:/var/lib/postgresql/data -p 54320:5432 postgres:11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-03 08:01:15.410 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n",
      "2019-08-03 08:01:15.410 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n",
      "2019-08-03 08:01:15.421 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n",
      "2019-08-03 08:01:15.451 UTC [26] LOG:  database system was shut down at 2019-08-03 08:00:51 UTC\n",
      "2019-08-03 08:01:15.467 UTC [1] LOG:  database system is ready to accept connections\n"
     ]
    }
   ],
   "source": [
    "# Checking the logs to see if it is running:\n",
    "!docker logs ht_pg_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP DATABASE\n",
      "CREATE DATABASE\n"
     ]
    }
   ],
   "source": [
    "# Creating the database:\n",
    "!docker exec -it ht_pg_server psql -U postgres -c \"drop database ht_db\"\n",
    "!docker exec -it ht_pg_server psql -U postgres -c \"create database ht_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import psycopg2 as pg\n",
      "import yaml\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "def create_tables(config, connection):\n",
      "    cur = connection.cursor()\n",
      "    for table in config:\n",
      "        name = table.get('name')\n",
      "        schema = table.get('schema')\n",
      "        ddl = f\"\"\"CREATE TABLE IF NOT EXISTS {name} ({schema})\"\"\"\n",
      "        cur.execute(ddl)\n",
      "\n",
      "    connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "def transform_tables(config):\n",
      "\n",
      "    data_path = \"../data/\"\n",
      "\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        table_cols = []\n",
      "        for i in table.get('columns'):\n",
      "            table_cols.append(str.upper(i))\n",
      "        df = pd.read_csv(table_source)\n",
      "        df_reorder = df[table_cols]  # rearrange column here\n",
      "        df_reorder.to_csv(table_source, index=False)\n",
      "\n",
      "def load_tables(config, connection):\n",
      "\n",
      "    # iterate and load\n",
      "    cur = connection.cursor()\n",
      "    data_path = \"../data/\"\n",
      "\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        with open(table_source, 'r') as f:\n",
      "            next(f)\n",
      "            cur.copy_expert(f\"COPY {table_name} FROM STDIN CSV NULL AS ''\", f)\n",
      "        connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "\n",
      "connection = pg.connect(\n",
      "    host='localhost',\n",
      "    port=54320,\n",
      "    dbname='ht_db',\n",
      "    user='postgres'\n",
      ")\n",
      "\n",
      "with open(\"../misc/schemas.yaml\") as schema_file:\n",
      "    config = yaml.load(schema_file)\n",
      "create_tables(config, connection)\n",
      "transform_tables(config)\n",
      "load_tables(config, connection)\n",
      "\n",
      "connection.close()"
     ]
    }
   ],
   "source": [
    "!cat ../code/etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "!python ../code/etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create features and store them in the DB\n",
      "\n",
      "import psycopg2 as pg\n",
      "import yaml\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "import dateutil\n",
      "\n",
      "\n",
      "def prepare_user_features(users_file_name):\n",
      "    user_df = pd.read_csv(users_file_name)\n",
      "    # There are NaN in TERMS_VERSION - fill it by \"Undefined\" version\n",
      "    user_df['TERMS_VERSION'] = user_df['TERMS_VERSION'].fillna(\"Undefined\")\n",
      "    user_df.drop('STATE', axis=1, inplace=True)\n",
      "    # Create AGE variable as age on CREATED_DATE date of profile in system\n",
      "    user_df['AGE'] = user_df.apply(lambda x: datetime.strptime(x['CREATED_DATE'], '%Y-%m-%d %H:%M:%S.%f').year-x['BIRTH_YEAR'], axis=1)\n",
      "    # We don't need BIRTH_YEAR anymore - drop it\n",
      "    user_df.drop('BIRTH_YEAR', axis=1, inplace=True)\n",
      "    return user_df\n",
      "\n",
      "\n",
      "def prepare_transactions_features(user_df, transactions_file_name):\n",
      "    trn_df = pd.read_csv(transactions_file_name)\n",
      "    df = trn_df.drop('ID', axis=1).merge(user_df, left_on=\"USER_ID\", right_on=\"ID\").drop(['ID'],axis=1)\n",
      "    # Replace NaN to 'Undefined' in MERCHANT_CATEGORY and MERCHANT_COUNTRY\n",
      "    df = df.fillna('Undefined')\n",
      "    # Create AGE variable as age on CREATED_DATE date of profile in system\n",
      "    df['PROFILE_AGE'] = df.apply(lambda x: (datetime.strptime(x['CREATED_DATE_x']+('000' if '.' in x['CREATED_DATE_x'] else '.000000'), '%Y-%m-%d %H:%M:%S.%f')-datetime.strptime(x['CREATED_DATE_y'], '%Y-%m-%d %H:%M:%S.%f')).days, axis=1)\n",
      "    df['CREATED_DATE_x'] = df['CREATED_DATE_x'].apply(lambda x: (datetime.strptime(x+('000' if '.' in x else '.000000'), '%Y-%m-%d %H:%M:%S.%f').strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
      "    # No needs of CREATED_DATE_y anymore, drop them\n",
      "    df.drop(['CREATED_DATE_y'], axis=1, inplace=True)\n",
      "    # Transform boolean variable IS_FRAUDSTER to binary\n",
      "    df['IS_FRAUDSTER'] = df['IS_FRAUDSTER'].astype(int)\n",
      "    return df\n",
      "    \n",
      "    \n",
      "def prepare_country_features(df, countries_file_name):\n",
      "    cntr_df = pd.read_csv(countries_file_name)\n",
      "    df['COUNTRY'] = df['COUNTRY'].map({c[0]: c[1] for c in cntr_df[['CODE','CODE3']].values})\n",
      "    # Trying to replace country code to CODE3\n",
      "    df['MERCHANT_COUNTRY'] = df['MERCHANT_COUNTRY'].apply(lambda x: (cntr_df[cntr_df['NUMCODE']==int(x)]['CODE3'].values[0] if len(cntr_df[cntr_df['NUMCODE']==int(x)]['CODE3'].values)>0 else x) if x.isdigit() else x)\n",
      "    # Trying to replace country code to CODE3\n",
      "    # Add \"C\" to the missing countries\n",
      "    df['MERCHANT_COUNTRY'] = df['MERCHANT_COUNTRY'].apply(lambda x: (cntr_df[cntr_df['PHONECODE']==int(x)]['CODE3'].values[0] if len(cntr_df[cntr_df['PHONECODE']==int(x)]['CODE3'].values)>0 else 'C'+x) if x.isdigit() else x)\n",
      "    # Fill NaN to Undefined and create binary variable HOMELAND to identify than transaction is made in country of user\n",
      "    df['COUNTRY'] = df['COUNTRY'].fillna('Undefined')\n",
      "    df['HOMELAND'] = df.apply(lambda x: 1 if x['COUNTRY'].upper() == x['MERCHANT_COUNTRY'].upper() else 0, axis=1)\n",
      "    return df\n",
      "\n",
      "\n",
      "def prepare_time_features(df, dataset_time_file_name):\n",
      "    # Functions to calculate day and hour of transactions of users\n",
      "    def transaction_by_day_count(dataset):\n",
      "        dataset['DAY_OF_TRANSACTION'] = dataset['CREATED_DATE_x'].apply(\n",
      "            lambda x: int((datetime.strptime(x, '%Y-%m-%d %H:%M:%S')).strftime('%w'))\n",
      "        ) \n",
      "        tmp_df = dataset.groupby(\n",
      "            ['USER_ID','DAY_OF_TRANSACTION']\n",
      "        )['DAY_OF_TRANSACTION'].size().unstack().fillna(0).reset_index()\n",
      "        old_columns = [\n",
      "            old_col for old_col in tmp_df.columns.tolist()\n",
      "            if old_col in dataset['DAY_OF_TRANSACTION'].unique()\n",
      "        ]\n",
      "        tmp_df.rename(\n",
      "            columns={old_col: 'DAY_' + str(old_col) for old_col in old_columns},\n",
      "            inplace=True\n",
      "        )\n",
      "        return tmp_df\n",
      "\n",
      "    def transaction_by_hour_count(dataset):\n",
      "        dataset['HOUR_OF_TRANSACTION'] = dataset['CREATED_DATE_x'].apply(\n",
      "            lambda x: int((datetime.strptime(x, '%Y-%m-%d %H:%M:%S')).strftime('%H'))\n",
      "        ) \n",
      "        tmp_df = dataset.groupby(\n",
      "            ['USER_ID','HOUR_OF_TRANSACTION']\n",
      "        )['HOUR_OF_TRANSACTION'].size().unstack().fillna(0).reset_index()\n",
      "        old_columns = [\n",
      "            old_col for old_col in tmp_df.columns.tolist()\n",
      "            if old_col in dataset['HOUR_OF_TRANSACTION'].unique()\n",
      "        ]\n",
      "        tmp_df.rename(\n",
      "            columns={old_col: 'H_' + str(old_col) for old_col in old_columns},\n",
      "            inplace=True\n",
      "        )\n",
      "        return tmp_df\n",
      "\n",
      "    transaction_by_day = transaction_by_day_count(df)\n",
      "    transaction_by_day = transaction_by_day.set_index('USER_ID')\n",
      "\n",
      "    transaction_by_hour = transaction_by_hour_count(df)\n",
      "    transaction_by_hour = transaction_by_hour.set_index('USER_ID')\n",
      "\n",
      "    # Join datasets\n",
      "    df_time = pd.merge(\n",
      "        transaction_by_day,\n",
      "        transaction_by_hour,\n",
      "        left_index=True,\n",
      "        right_index=True,)\n",
      "\n",
      "    df_time = df_time.reset_index()\n",
      "    df_time.to_csv(dataset_time_file_name)\n",
      "    return df\n",
      "\n",
      "\n",
      "def prepare_currency_features(df, currencies_file_name):\n",
      "    cur_df = pd.read_csv(currencies_file_name)\n",
      "    df = pd.merge(df, cur_df, left_on=\"CURRENCY\", right_on=\"CCY\")\n",
      "    df['IS_CRYPTO'] = df['IS_CRYPTO'].astype(int)\n",
      "    df['AMOUNT'] = df['AMOUNT'] * 10**df['EXPONENT']\n",
      "    return df\n",
      "\n",
      "\n",
      "def collect_all_features(df, dataset_file_name):\n",
      "    cols_to_select = ['USER_ID', 'CURRENCY', 'AMOUNT',\n",
      "           'STATE', 'MERCHANT_CATEGORY', 'ENTRY_METHOD', 'TYPE', 'SOURCE', 'HAS_EMAIL', \n",
      "           'IS_FRAUDSTER', 'TERMS_VERSION', 'KYC', 'FAILED_SIGN_IN_ATTEMPTS', 'AGE', 'PROFILE_AGE', 'HOMELAND',\n",
      "           'DAY_OF_TRANSACTION', 'HOUR_OF_TRANSACTION', 'IS_CRYPTO']\n",
      "    df = df[cols_to_select]\n",
      "    df.to_csv(dataset_file_name)\n",
      "    \n",
      "\n",
      "def create_tables(config, connection):\n",
      "    cur = connection.cursor()\n",
      "    for table in config:\n",
      "        name = table.get('name')\n",
      "        schema = table.get('schema')\n",
      "        ddl = f\"\"\"CREATE TABLE IF NOT EXISTS {name} ({schema})\"\"\"\n",
      "        cur.execute(ddl)\n",
      "    connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "    \n",
      "def transform_tables(config, data_path = \"../data/\"):\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        table_cols = []\n",
      "        for i in table.get('columns'):\n",
      "            table_cols.append(str.upper(i))\n",
      "        df = pd.read_csv(table_source)\n",
      "        df_reorder = df[table_cols]  # rearrange column here\n",
      "        df_reorder.to_csv(table_source, index=False)\n",
      "\n",
      "        \n",
      "def load_tables(config, connection, data_path = \"../data/\"):\n",
      "    # iterate and load\n",
      "    cur = connection.cursor()\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        with open(table_source, 'r') as f:\n",
      "            next(f)\n",
      "            cur.copy_expert(f\"COPY {table_name} FROM STDIN CSV NULL AS ''\", f)\n",
      "        connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "    \n",
      "def store_features_to_db(yaml_file, host, port, dbname, user):\n",
      "    # open connection\n",
      "    connection = pg.connect(\n",
      "        host = host,\n",
      "        port = port,\n",
      "        dbname = dbname,\n",
      "        user = user\n",
      "    )\n",
      "    # open config file and store files to the DB\n",
      "    with open(yaml_file) as schema_file:\n",
      "        config = yaml.load(schema_file)\n",
      "    create_tables(config, connection)\n",
      "    transform_tables(config)\n",
      "    load_tables(config, connection)\n",
      "    # close connection\n",
      "    connection.close()\n",
      "\n",
      "    \n",
      "if __name__ == \"__main__\":\n",
      "    user_df = prepare_user_features(users_file_name = '../data/users.csv')\n",
      "    df = prepare_transactions_features(user_df, transactions_file_name = '../data/transactions.csv')\n",
      "    df = prepare_country_features(df, countries_file_name = '../data/countries.csv')\n",
      "    df = prepare_time_features(df, dataset_time_file_name = '../data/dataset_time.csv')\n",
      "    df = prepare_currency_features(df, currencies_file_name = '../data/currency_details.csv')\n",
      "    df = collect_all_features(df, dataset_file_name = '../data/dataset.csv')\n",
      "\n",
      "    store_features_to_db(yaml_file = \"../misc/schemas_features.yaml\",\n",
      "                         host='localhost',\n",
      "                         port=54320,\n",
      "                         dbname='ht_db',\n",
      "                         user='postgres')"
     ]
    }
   ],
   "source": [
    "!cat ../code/features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "../code/features.py:64: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if old_col in dataset['DAY_OF_TRANSACTION'].unique()\n",
      "../code/features.py:81: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if old_col in dataset['HOUR_OF_TRANSACTION'].unique()\n"
     ]
    }
   ],
   "source": [
    "!python ../code/features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
