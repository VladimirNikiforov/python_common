{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vladimir Nikiforov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 0 - PREPARE ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ht_pg_server\n",
      "29896e0c0fff\n",
      "Untagged: postgres:11\n",
      "Untagged: postgres@sha256:68b49a280d2fbe9330c0031970ebb72015e1272dfa25f0ed7557514f9e5ad7b7\n",
      "Deleted: sha256:53912975086f1470f877922e3eb79c4f17c403fcb0b51b1f45df9b623987600e\n",
      "Deleted: sha256:859135cc89a4161becb56082dc95772d26d157bc67622be28ccab159cee9c520\n",
      "Deleted: sha256:85d5ba072f703aed8ed136ee24ef0a5c22fb818e307db40be0637df1691546fe\n",
      "Deleted: sha256:44bc37a16317acf120ebcb0b8aa2cf8738f0dcc40ca47f18d9d5789099089cd0\n",
      "Deleted: sha256:050e2fbc4c511fff109624929dcc24d61f4565052ffd6e30e3d46d42ce65c576\n",
      "Deleted: sha256:6e7901ce413d382c6544cf07b5370341507a4734525947ecef28944c041e39b6\n",
      "Deleted: sha256:c0d5a0e28851bc9a301c0cb92e03c036c1a4b0d8da1efc4a16a921abdd641e3e\n",
      "Deleted: sha256:21cf04dd5b4e7ddbe4a7f122b532038c351eef9057ff71940a1a0a36069fabd5\n",
      "Deleted: sha256:5eba02d7eb8edfd2d0c4503cd6976cfb526e96c4f62960bac5efdf86a1e71e4f\n",
      "Deleted: sha256:0e0ff67afa7921baa6b3c7afd83664752663b5b8704c5236941359003e6f8887\n",
      "Deleted: sha256:915e9e2c13848dc5933cea4c23b53649971b434b1dc1a8ca208f3f47f86ceab1\n",
      "Deleted: sha256:b36245f574fc056b60e016e9db27440a7daac357cf2cc1eb2407a791b01d5ea0\n",
      "Deleted: sha256:33caa0d1d5a95acf62a0767e830c2f656d652771edd7b8c7b04bf97af98ba40f\n",
      "Deleted: sha256:c2c9479f4b920b4454d1b077424280197a3ee03a4c157fec41427af7683dce95\n",
      "Deleted: sha256:d56055da3352f918f4d8a42350385ea5b10d0906e746a8fbb4b850f9284deee5\n"
     ]
    }
   ],
   "source": [
    "# Stop previously runned server\n",
    "!docker stop ht_pg_server\n",
    "!docker rm $(docker ps -a -q)\n",
    "!docker rmi $(docker images -q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find image 'postgres:11' locally\n",
      "11: Pulling from library/postgres\n",
      "\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[1B\n",
      "\u001b[10B\n",
      "\u001b[1BDigest: sha256:68b49a280d2fbe9330c0031970ebb72015e1272dfa25f0ed7557514f9e5ad7b7\n",
      "Status: Downloaded newer image for postgres:11\n",
      "edb79af76f1c6cd62c0e974d6173d54e4c99643d8b37b0a6c4a89fddee0ac5ca\n"
     ]
    }
   ],
   "source": [
    "# Run the server:\n",
    "!docker run -d --name ht_pg_server -v ht_dbdata:/var/lib/postgresql/data -p 54320:5432 postgres:11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-04 12:53:49.761 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n",
      "2019-08-04 12:53:49.761 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n",
      "2019-08-04 12:53:49.773 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n",
      "2019-08-04 12:53:49.792 UTC [30] LOG:  database system was shut down at 2019-08-04 12:53:17 UTC\n",
      "2019-08-04 12:53:49.799 UTC [1] LOG:  database system is ready to accept connections\n"
     ]
    }
   ],
   "source": [
    "# Checking the logs to see if it is running:\n",
    "!docker logs ht_pg_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DROP DATABASE\n",
      "CREATE DATABASE\n"
     ]
    }
   ],
   "source": [
    "# Creating the database:\n",
    "!docker exec -it ht_pg_server psql -U postgres -c \"drop database ht_db\"\n",
    "!docker exec -it ht_pg_server psql -U postgres -c \"create database ht_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# change working directory to the directory with code\n",
    "os.chdir('../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import psycopg2 as pg\n",
      "import yaml\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "def create_tables(config, connection):\n",
      "    cur = connection.cursor()\n",
      "    for table in config:\n",
      "        name = table.get('name')\n",
      "        schema = table.get('schema')\n",
      "        ddl = f\"\"\"CREATE TABLE IF NOT EXISTS {name} ({schema})\"\"\"\n",
      "        cur.execute(ddl)\n",
      "\n",
      "    connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "def transform_tables(config):\n",
      "\n",
      "    data_path = \"../data/\"\n",
      "\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        table_cols = []\n",
      "        for i in table.get('columns'):\n",
      "            table_cols.append(str.upper(i))\n",
      "        df = pd.read_csv(table_source)\n",
      "        df_reorder = df[table_cols]  # rearrange column here\n",
      "        df_reorder.to_csv(table_source, index=False)\n",
      "\n",
      "def load_tables(config, connection):\n",
      "\n",
      "    # iterate and load\n",
      "    cur = connection.cursor()\n",
      "    data_path = \"../data/\"\n",
      "\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        with open(table_source, 'r') as f:\n",
      "            next(f)\n",
      "            cur.copy_expert(f\"COPY {table_name} FROM STDIN CSV NULL AS ''\", f)\n",
      "        connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "\n",
      "connection = pg.connect(\n",
      "    host='localhost',\n",
      "    port=54320,\n",
      "    dbname='ht_db',\n",
      "    user='postgres'\n",
      ")\n",
      "\n",
      "with open(\"../misc/schemas.yaml\") as schema_file:\n",
      "    config = yaml.load(schema_file)\n",
      "create_tables(config, connection)\n",
      "transform_tables(config)\n",
      "load_tables(config, connection)\n",
      "\n",
      "connection.close()"
     ]
    }
   ],
   "source": [
    "!cat etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "# execute ETL-script\n",
    "!python etl.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# create features and store them in the DB\n",
      "\n",
      "import psycopg2 as pg\n",
      "import yaml\n",
      "from pathlib import Path\n",
      "import os\n",
      "import pandas as pd\n",
      "from datetime import datetime\n",
      "import dateutil\n",
      "\n",
      "\n",
      "def prepare_user_features(users_file_name):\n",
      "    user_df = pd.read_csv(users_file_name)\n",
      "    # There are NaN in TERMS_VERSION - fill it by \"Undefined\" version\n",
      "    user_df['TERMS_VERSION'] = user_df['TERMS_VERSION'].fillna(\"Undefined\")\n",
      "    user_df.drop('STATE', axis=1, inplace=True)\n",
      "    # Create AGE variable as age on CREATED_DATE date of profile in system\n",
      "    user_df['AGE'] = user_df.apply(lambda x: datetime.strptime(x['CREATED_DATE'], '%Y-%m-%d %H:%M:%S.%f').year-x['BIRTH_YEAR'], axis=1)\n",
      "    # We don't need BIRTH_YEAR anymore - drop it\n",
      "    user_df.drop('BIRTH_YEAR', axis=1, inplace=True)\n",
      "    return user_df\n",
      "\n",
      "\n",
      "def prepare_transactions_features(user_df, transactions_file_name):\n",
      "    trn_df = pd.read_csv(transactions_file_name)\n",
      "    df = trn_df.drop('ID', axis=1).merge(user_df, left_on=\"USER_ID\", right_on=\"ID\").drop(['ID'],axis=1)\n",
      "    # Replace NaN to 'Undefined' in MERCHANT_CATEGORY and MERCHANT_COUNTRY\n",
      "    df = df.fillna('Undefined')\n",
      "    # Create AGE variable as age on CREATED_DATE date of profile in system\n",
      "    df['PROFILE_AGE'] = df.apply(lambda x: (datetime.strptime(x['CREATED_DATE_x']+('000' if '.' in x['CREATED_DATE_x'] else '.000000'), '%Y-%m-%d %H:%M:%S.%f')-datetime.strptime(x['CREATED_DATE_y'], '%Y-%m-%d %H:%M:%S.%f')).days, axis=1)\n",
      "    df['CREATED_DATE_x'] = df['CREATED_DATE_x'].apply(lambda x: (datetime.strptime(x+('000' if '.' in x else '.000000'), '%Y-%m-%d %H:%M:%S.%f').strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
      "    # No needs of CREATED_DATE_y anymore, drop them\n",
      "    df.drop(['CREATED_DATE_y'], axis=1, inplace=True)\n",
      "    # Transform boolean variable IS_FRAUDSTER to binary\n",
      "    df['IS_FRAUDSTER'] = df['IS_FRAUDSTER'].astype(int)\n",
      "    return df\n",
      "    \n",
      "    \n",
      "def prepare_country_features(df, countries_file_name):\n",
      "    cntr_df = pd.read_csv(countries_file_name)\n",
      "    # Trying to replace country code to CODE3\n",
      "    df['COUNTRY'] = df['COUNTRY'].map({c[0]: c[1] for c in cntr_df[['CODE','CODE3']].values})\n",
      "    # Trying to replace country code to CODE3\n",
      "    df['MERCHANT_COUNTRY'] = df['MERCHANT_COUNTRY'].apply(lambda x: (cntr_df[cntr_df['NUMCODE']==int(x)]['CODE3'].values[0] if len(cntr_df[cntr_df['NUMCODE']==int(x)]['CODE3'].values)>0 else x) if x.isdigit() else x)\n",
      "    # Trying to replace country code to CODE3 and add \"C\" to the missing countries to set string format\n",
      "    df['MERCHANT_COUNTRY'] = df['MERCHANT_COUNTRY'].apply(lambda x: (cntr_df[cntr_df['PHONECODE']==int(x)]['CODE3'].values[0] if len(cntr_df[cntr_df['PHONECODE']==int(x)]['CODE3'].values)>0 else 'C'+x) if x.isdigit() else x)\n",
      "    # Fill NaN to Undefined and create binary variable HOMELAND to identify than transaction is made in country of user\n",
      "    df['COUNTRY'] = df['COUNTRY'].fillna('Undefined')\n",
      "    # Feature HOMELAND to check that transaction is made from user' homeland country\n",
      "    df['HOMELAND'] = df.apply(lambda x: 1 if x['COUNTRY'].upper() == x['MERCHANT_COUNTRY'].upper() else 0, axis=1)\n",
      "    return df\n",
      "\n",
      "\n",
      "def prepare_time_features(df, dataset_time_file_name):\n",
      "    # Functions to calculate day and hour of transactions of users\n",
      "    def transaction_by_day_count(dataset):\n",
      "        # create features in transactions (DAY_OF_TRANSACTION) and users (number of transactions for each day of week)\n",
      "        dataset['DAY_OF_TRANSACTION'] = dataset['CREATED_DATE_x'].apply(\n",
      "            lambda x: int((datetime.strptime(x, '%Y-%m-%d %H:%M:%S')).strftime('%w'))\n",
      "        ) \n",
      "        tmp_df = dataset.groupby(\n",
      "            ['USER_ID','DAY_OF_TRANSACTION']\n",
      "        )['DAY_OF_TRANSACTION'].size().unstack().fillna(0).reset_index()\n",
      "        old_columns = [\n",
      "            old_col for old_col in tmp_df.columns.tolist()\n",
      "            if old_col in dataset['DAY_OF_TRANSACTION'].unique()\n",
      "        ]\n",
      "        tmp_df.rename(\n",
      "            columns={old_col: 'DAY_' + str(old_col) for old_col in old_columns},\n",
      "            inplace=True\n",
      "        )\n",
      "        return tmp_df\n",
      "\n",
      "    def transaction_by_hour_count(dataset):\n",
      "        # create features in transactions (HOUR_OF_TRANSACTION) and users (number of transactions for each hour in day)\n",
      "        dataset['HOUR_OF_TRANSACTION'] = dataset['CREATED_DATE_x'].apply(\n",
      "            lambda x: int((datetime.strptime(x, '%Y-%m-%d %H:%M:%S')).strftime('%H'))\n",
      "        ) \n",
      "        tmp_df = dataset.groupby(\n",
      "            ['USER_ID','HOUR_OF_TRANSACTION']\n",
      "        )['HOUR_OF_TRANSACTION'].size().unstack().fillna(0).reset_index()\n",
      "        old_columns = [\n",
      "            old_col for old_col in tmp_df.columns.tolist()\n",
      "            if old_col in dataset['HOUR_OF_TRANSACTION'].unique()\n",
      "        ]\n",
      "        tmp_df.rename(\n",
      "            columns={old_col: 'H_' + str(old_col) for old_col in old_columns},\n",
      "            inplace=True\n",
      "        )\n",
      "        return tmp_df\n",
      "\n",
      "    transaction_by_day = transaction_by_day_count(df)\n",
      "    transaction_by_day = transaction_by_day.set_index('USER_ID')\n",
      "\n",
      "    transaction_by_hour = transaction_by_hour_count(df)\n",
      "    transaction_by_hour = transaction_by_hour.set_index('USER_ID')\n",
      "\n",
      "    # Join datasets\n",
      "    df_time = pd.merge(\n",
      "        transaction_by_day,\n",
      "        transaction_by_hour,\n",
      "        left_index=True,\n",
      "        right_index=True,)\n",
      "\n",
      "    df_time = df_time.reset_index()\n",
      "    # we will store time-features separetely from main dataset to minimize space on disk and database\n",
      "    df_time.to_csv(dataset_time_file_name)\n",
      "    return df\n",
      "\n",
      "\n",
      "def prepare_currency_features(df, currencies_file_name):\n",
      "    # merge currency dict to the complete dataframe to modify amounts and add feature IS_CRYPTO\n",
      "    cur_df = pd.read_csv(currencies_file_name)\n",
      "    df = pd.merge(df, cur_df, left_on=\"CURRENCY\", right_on=\"CCY\")\n",
      "    df['IS_CRYPTO'] = df['IS_CRYPTO'].astype(int)\n",
      "    # apply exponents to the amount\n",
      "    df['AMOUNT'] = df['AMOUNT'] * 10**df['EXPONENT']\n",
      "    return df\n",
      "\n",
      "\n",
      "def collect_all_features(df, dataset_file_name):\n",
      "    # Filter only necessary features and store dataframe to the file\n",
      "    cols_to_select = ['USER_ID', 'AMOUNT',\n",
      "           'STATE', 'MERCHANT_CATEGORY', 'ENTRY_METHOD', 'TYPE', 'SOURCE', 'HAS_EMAIL', \n",
      "           'IS_FRAUDSTER', 'TERMS_VERSION', 'KYC', 'FAILED_SIGN_IN_ATTEMPTS', 'AGE', 'PROFILE_AGE', 'HOMELAND',\n",
      "           'DAY_OF_TRANSACTION', 'HOUR_OF_TRANSACTION', 'IS_CRYPTO']\n",
      "    df = df[cols_to_select]\n",
      "    df.to_csv(dataset_file_name)\n",
      "    \n",
      "\n",
      "def create_tables(config, connection):\n",
      "    cur = connection.cursor()\n",
      "    for table in config:\n",
      "        name = table.get('name')\n",
      "        schema = table.get('schema')\n",
      "        ddl = f\"\"\"CREATE TABLE IF NOT EXISTS {name} ({schema})\"\"\"\n",
      "        cur.execute(ddl)\n",
      "    connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "    \n",
      "def transform_tables(config, data_path = \"../data/\"):\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        table_cols = []\n",
      "        for i in table.get('columns'):\n",
      "            table_cols.append(str.upper(i))\n",
      "        df = pd.read_csv(table_source)\n",
      "        df_reorder = df[table_cols]  # rearrange column here\n",
      "        df_reorder.to_csv(table_source, index=False)\n",
      "\n",
      "        \n",
      "def load_tables(config, connection, data_path = \"../data/\"):\n",
      "    # iterate and load\n",
      "    cur = connection.cursor()\n",
      "    for table in config:\n",
      "        table_name = table.get('name')\n",
      "        table_source = os.path.join(data_path,f\"{table_name}.csv\")\n",
      "        with open(table_source, 'r') as f:\n",
      "            next(f)\n",
      "            cur.copy_expert(f\"COPY {table_name} FROM STDIN CSV NULL AS ''\", f)\n",
      "        connection.commit()\n",
      "    cur.close()\n",
      "\n",
      "    \n",
      "def store_features_to_db(yaml_file, host, port, dbname, user):\n",
      "    # open connection\n",
      "    connection = pg.connect(\n",
      "        host = host,\n",
      "        port = port,\n",
      "        dbname = dbname,\n",
      "        user = user\n",
      "    )\n",
      "    # open config file and store files to the DB\n",
      "    with open(yaml_file) as schema_file:\n",
      "        config = yaml.load(schema_file)\n",
      "    create_tables(config, connection)\n",
      "    transform_tables(config)\n",
      "    load_tables(config, connection)\n",
      "    # close connection\n",
      "    connection.close()\n",
      "\n",
      "    \n",
      "if __name__ == \"__main__\":\n",
      "    # load users dataset and prepare features\n",
      "    user_df = prepare_user_features(users_file_name = '../data/users.csv')\n",
      "    # join users dataset with transactions and add new features\n",
      "    df = prepare_transactions_features(user_df, transactions_file_name = '../data/transactions.csv')\n",
      "    # apply countries' dataset to get single format of countries\n",
      "    df = prepare_country_features(df, countries_file_name = '../data/countries.csv')\n",
      "    # get period and time features, apply to the total dataframe and store them to the file\n",
      "    df = prepare_time_features(df, dataset_time_file_name = '../data/dataset_time.csv')\n",
      "    # apply currencies' rules and add new features\n",
      "    df = prepare_currency_features(df, currencies_file_name = '../data/currency_details.csv')\n",
      "    # filter features and store total dataframe to the file\n",
      "    df = collect_all_features(df, dataset_file_name = '../data/dataset.csv')\n",
      "    # save total and time dataframes to the database\n",
      "    store_features_to_db(yaml_file = \"../misc/schemas_features.yaml\",\n",
      "                         host='localhost',\n",
      "                         port=54320,\n",
      "                         dbname='ht_db',\n",
      "                         user='postgres')"
     ]
    }
   ],
   "source": [
    "!cat features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n",
      "features.py:66: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if old_col in dataset['DAY_OF_TRANSACTION'].unique()\n",
      "features.py:84: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if old_col in dataset['HOUR_OF_TRANSACTION'].unique()\n"
     ]
    }
   ],
   "source": [
    "# execute script to create features and store them to the DB\n",
    "!python features.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generates the fitted model artifact\n",
      "\n",
      "def load_datasets(dataset_file_name, dataset_time_file_name):\n",
      "    # Load and merge prepared datasets to the final dataset\n",
      "    \n",
      "    import pandas as pd\n",
      "\n",
      "    # We don't need CURRENCY as a feature because FRAUDSTER is not identified by CURRENCY\n",
      "    df = pd.read_csv(dataset_file_name)\n",
      "    df_time = pd.read_csv(dataset_time_file_name)\n",
      "\n",
      "    # Let's create dummy columns for categorical variables\n",
      "    df = pd.get_dummies(df, columns=['ENTRY_METHOD','TYPE','SOURCE','STATE','MERCHANT_CATEGORY','TERMS_VERSION','KYC'])\n",
      "    # and collect final dataset with time-features from dataset_time\n",
      "    df = df.merge(df_time, left_on=\"USER_ID\", right_on=\"USER_ID\")\n",
      "    del df_time\n",
      "    return df\n",
      "\n",
      "\n",
      "def oversampling_dataset(df):\n",
      "    # Do oversampling of unbalanced class\n",
      "    \n",
      "    import numpy as np\n",
      "    # split dataset to features and predicted value\n",
      "    X, y = df.drop('IS_FRAUDSTER',axis=1), df['IS_FRAUDSTER']\n",
      "\n",
      "    # balancing classes\n",
      "    classes_count = y.value_counts()\n",
      "    max_class_count = classes_count.max()\n",
      "\n",
      "    X_over = X.copy()\n",
      "    y_over = y.copy()\n",
      "\n",
      "    for cls in zip(classes_count,classes_count.index):\n",
      "        if cls[0] != max_class_count:\n",
      "            idx = np.random.choice(y[y==cls[1]].index,size = max_class_count-cls[0])\n",
      "            X_add = X.iloc[idx]\n",
      "            y_add = y.iloc[idx]\n",
      "            X_over = X_over.append(X_add)\n",
      "            y_over = y_over.append(y_add)\n",
      "\n",
      "    del X,y\n",
      "    return X_over, y_over\n",
      "\n",
      "\n",
      "def train_and_save_model(X_over, y_over, model_file_name, test_users_file_name, N_FOLDS, RANDOM_STATE):\n",
      "    # Train and save model to the file\n",
      "    \n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "    from sklearn.model_selection import train_test_split, KFold\n",
      "    \n",
      "    # split to train and test datasets\n",
      "    X_train_over,X_test_over,y_train_over,_ = train_test_split(X_over,y_over,test_size=.2, random_state=RANDOM_STATE)\n",
      "\n",
      "    # store user_id of untrained users to check model\n",
      "    users_to_check = list(set(X_test_over['USER_ID'].values).difference(set(X_train_over['USER_ID'].values)))\n",
      "    X_train_over.drop('USER_ID', axis=1, inplace=True)\n",
      "    # train simple randomforest with KFold\n",
      "    rf = RandomForestClassifier(n_jobs=-1,random_state=RANDOM_STATE)\n",
      "    rf.fit(X_train_over,y_train_over)\n",
      "    # store list of features to the model parameter\n",
      "    rf.feature_names = list(X_train_over.columns.values)\n",
      "    \n",
      "    # save model to artifact\n",
      "    from sklearn.externals import joblib\n",
      "    joblib.dump(rf, model_file_name)\n",
      "    # save list of untrained users to check the model\n",
      "    joblib.dump(users_to_check, test_users_file_name)\n",
      "    del rf\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # load datasets from files\n",
      "    df = load_datasets(dataset_file_name = '../data/dataset.csv', dataset_time_file_name = '../data/dataset_time.csv')\n",
      "    # oversampling fraudsters' class for better performance\n",
      "    X_over, y_over = oversampling_dataset(df)\n",
      "    # train and save model to the file\n",
      "    train_and_save_model(X_over, y_over, model_file_name = '../artifacts/model.pkl', test_users_file_name = '../data/test_users.pkl', N_FOLDS = 3, RANDOM_STATE = 7)    "
     ]
    }
   ],
   "source": [
    "!cat train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n",
      "/home/truename/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/home/truename/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# execute script to train the model\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Load and identify users by pretrained model\n",
      "\n",
      "def get_user_data(user_id, model_features, cat_features):\n",
      "    # Load data from DB for specific user_id, split cat_features to dummies and fill absented categorical features' values by zeros to create complete dataset with model_features\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    import psycopg2\n",
      "    # Create connection to DB\n",
      "    connection = psycopg2.connect(\n",
      "        host='localhost',\n",
      "        port=54320,\n",
      "        dbname='ht_db',\n",
      "        user='postgres',\n",
      "    )\n",
      "    # open cursor\n",
      "    c = connection.cursor()\n",
      "    # load dataset for specific user_id\n",
      "    c.execute(f\"SELECT * FROM dataset t1, dataset_time t2 where t1.USER_ID = t2.USER_ID and t1.USER_ID = '{user_id}';\")\n",
      "    # get all rows from opened cursor\n",
      "    result = c.fetchall()\n",
      "    # check that we have one or more rows\n",
      "    assert len(result) > 0, f\"We don't have data for user_id={user_id}!\"\n",
      "    # get column' names from cursor to create dataframe\n",
      "    colnames = [desc[0].upper() for desc in c.description]\n",
      "    # close cursor\n",
      "    c.close()\n",
      "    # create dataframe\n",
      "    result = pd.DataFrame(result, columns = colnames)\n",
      "    # split categorical features from dataset to dummies\n",
      "    result = pd.get_dummies(result, columns = cat_features)\n",
      "    # absented fields with categorical features' values fill with zeros and concat with loaded dataset to create complete dataset with model_features\n",
      "    result = pd.concat([result,pd.DataFrame(0, index=np.arange(len(result)), columns=[x for x in model_features if x not in result.columns])], axis = 1)[model_features]\n",
      "    # Close connection to DB\n",
      "    connection.close()\n",
      "    return result\n",
      "\n",
      "def check_alert(y_predicted):\n",
      "    # Return the most important (heaviest by weight) flag as a result\n",
      "    '''\n",
      "    Rules:\n",
      "        If percent is more than first level (50% for example) we need ALERT (weight=1) because it's suspicious transaction.\n",
      "        If percent is more than second level (75% for example) we need LOCK and ALERT (weight=2) because it's very suspicious transaction and it's better to lock user and send alert signal to work with this user.\n",
      "        If percent is more than max level (90% for example) we need LOCK (weight=3) because it's fraudster.\n",
      "    '''\n",
      "    \n",
      "    # dictionary of alerts\n",
      "    dict_of_alerts = {0: ['PASS'],\n",
      "                      1: ['ALERT_AGENT'],\n",
      "                      2: ['LOCK_USER', 'ALERT_AGENT'],\n",
      "                      3: ['LOCK_USER']\n",
      "                     }\n",
      "    # for each prediction in y_prediction check the rules, get the max weight and apply dictionary to get the alert\n",
      "    return dict_of_alerts[max([{       y >= .9: 3,\n",
      "                                .75 <= y <  .9: 2,\n",
      "                                .5  <= y < .75: 1,\n",
      "                                       y <  .5: 0}[True] for y in y_predicted])]\n",
      "\n",
      "def load_model(model_path):\n",
      "    # Load pickled model from artifact\n",
      "    from sklearn.externals import joblib\n",
      "    rf = joblib.load(model_path)\n",
      "    return rf\n",
      "\n",
      "\n",
      "def patrol(user_id):\n",
      "    # Main function\n",
      "    cat_features = ['ENTRY_METHOD','TYPE','SOURCE','STATE','MERCHANT_CATEGORY','TERMS_VERSION','KYC']\n",
      "    \n",
      "    # get list of features from the model\n",
      "    try:\n",
      "        col_list = rf.feature_names\n",
      "    except NameError:\n",
      "        # if model is not loaded yet => load our pretrained model\n",
      "        rf = load_model(model_path = '../artifacts/model.pkl')\n",
      "        col_list = rf.feature_names\n",
      "\n",
      "    # load data for user_id\n",
      "    X = get_user_data(user_id = user_id, model_features = col_list, cat_features = cat_features)\n",
      "\n",
      "    # get prediction of model\n",
      "    y_pred = rf.predict_proba(X)[:,1]\n",
      "\n",
      "    # apply rules and return solution\n",
      "    return check_alert(y_pred)"
     ]
    }
   ],
   "source": [
    "!cat patrol.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import function \"patrol\" from script \"patrol.py\"\n",
    "from patrol import patrol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/truename/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/home/truename/anaconda3/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['PASS']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Not a fraudster\n",
    "patrol(user_id='1872820f-e3ac-4c02-bdc7-727897b60043')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LOCK_USER']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Fraudster\n",
    "patrol(user_id='fb23710b-609a-49bf-8a9a-be49c59ce6de')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
